<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BLIP-2 - Querying Transformer (Q-Former)</title>
    <url>/blogs/2024/07/22/BLIP-2-Querying-Transformer-Q-Former/</url>
    <content><![CDATA[<h2 id="BLIP-2-Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/d40702382fcc928ef5e03a04addaf925.png" alt=""></p>
<h3 id="Motivation">Motivation</h3>
<p>Large image models and large language models were trained separately, and it might be difficult for them to directly communicate to do multimodal tasks, like Visual Question Answsering (VQA) and mutimodal retrieval.</p>
<p>Multimodal training is computational costly, and fine-tuning from existing seprated unimodal large models would cause catastrophic forgetting.</p>
<p>So the paper proposes Q-Former to connect them, such that during training, the separate unimodal models are frozen, but only to train the Q-Former to get text-aligned visual features that facilitate multimodal tasks.</p>
<h3 id="Training">Training</h3>
<p>Q-Former is trained in two stages. On the first stage, the purpose of training is that given a pretrained image encoder and a text, to make Q-Former output tokens that align with the given text. On the second stage, the output of Q-Former is connected to LLMs as “soft prompt” such that LLMs can generate related text.</p>
<h4 id="1-First-stage">1. First stage</h4>
<p><img src="/blogs/joplin_resources/e56d5c6059062e0f2bd78e84a4e9e2ac.png" alt=""></p>
<p>So the big image encoder is frozen, and Q-former consists of two transformer branches as shown above with <strong>shared self-attention paramters</strong>.  The queries of the vision branch are learnable (32 * 768, 32 queries with dimension of 768). In the Cro-attention layer of the vision transformer, keys and values are from the frozen image encoder. The training of Q-Former involves three objectives.</p>
<ul>
<li>ITC (Image-Text Contrastive learning)<br>
In the self-attention layer, uni-modal mask is adopted, which means the visual queries can only att end to other visual queries (because it’s self-attention, so queries here are regarded as tokens), not text tokens, and text tokens can also only attend to other text tokens. Like CLIP, a contrastive learning objective is used to align the output of the two transformers. Since the Q-Former costs much less memory than typical multimodal learning methods, so they didn’t use momentum queue in BLIP, but in-batch negatives.</li>
<li>ITG (Image-grounded Text Generation)<br>
So here, in the self-attention layer, the multi-modal causal mask is used, such that visual tokens only attend to other visual tokens, but text tokens attend to all the visual tokens and its previous text tokens. The training objective is a next token prediction decoding task.</li>
<li>ITM (Image Text Matching)<br>
In the self-attention layer, the bi-directional mask is used like in BERT, and compute the similarities between the cls token in the text transformer output and each of the queries in the vision transformer output, getting 32 similarities, and use the biggest similarity score as the matching score. They also use hard negative mining strategy as used in BLIP to create informative negative pairs.</li>
</ul>
<h4 id="2-Second-stage">2. Second stage</h4>
<p><img src="/blogs/joplin_resources/8e53c36be42f4448cea55d5621511477.png" alt=""></p>
<p>The second stage is quite simple, as we also get the visual prompt from Q-Former (this is done by using FC to transform the dimension of the Q-Former otuput tokens of the vision branch to be compatible with the text tokens for the LLMs). In this stage, decoder-based training or encoder-decoder-based trainin can be used as shown above.</p>
<h3 id="The-advantes-of-BLIP2">The advantes of BLIP2</h3>
<ol>
<li>It uses less memory and computation and leverages pre-trained LLMs and large vision models for multimodal learning, without fine-tuning LLMs and large vision models. It acts like an adapter to bridge to make multi modals aligned.</li>
<li>It can do VQA, it can create dialogue between images and humans through text.</li>
</ol>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Multimodal learning</tag>
        <tag>Large vision models</tag>
      </tags>
  </entry>
  <entry>
    <title>Going with small and fast networks (1)</title>
    <url>/blogs/2019/06/16/Going-with-small-and-fast-networks-1/</url>
    <content><![CDATA[<h2 id="Overview">Overview</h2>
<p>In this post, we are going to look at the following neural network models: MobileNet v1<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> &amp; v2<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, SqueezeNet<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, ShuffleNet v1<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> &amp; v2<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>, NasNet<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>. We consider the following questions:</p>
<span id="more"></span>
<ol>
<li>
<p>What in the world do they look like?</p>
</li>
<li>
<p>Why are they fast? Why are they small? Which one is better and Why?</p>
</li>
<li>
<p>Why the authors design them like that?</p>
</li>
</ol>
<p>So, let’s try to solve these doubts step by step.</p>
<h2 id="MobileNet-v1-vs-Standard-CNN-models">MobileNet v1 vs. Standard CNN models</h2>
<p>MobileNet v1 is smart enough to decompose the standard convolution operation into two separate operations: depth-wise (or channel-wise) convolution and point-wise convolution.</p>
<p>We can take the following figure as an illustration:</p>
<img src="mobilenetv1.png" width="400">
<p>Suppose we have the convolutional layer with kernel size $K$, input size $C_{in}\times H\times W$ and output size $C_{out} \times H \times W$ (stride=1). For a standard convolution operation, the computation complexity, here we use MACC (Multiply-accumulate, also known as MADD), is calculated as (for how to calculate FLOPs or MACC, we kindly recommend this great post: <a href="https://machinethink.net/blog/how-fast-is-my-model/">How Fast is my model?</a>):</p>
<p>$$\begin{equation}\label{eq1}<br>
K\times K\times C_{in}\times C_{out}\times H\times W.<br>
\end{equation}$$</p>
<p>With decomposition, the two separate operation parts lead to output feature maps with exactly the same size as the standard counterpart does while with much less computation cost. How does that works?</p>
<p>OK, depth-wise convolution takes as input a single channel and output a single channel for each channel of the input volume, and then concatenates the output channels for the second stage, in which the point-wise convolution takes place. According to this, its corresponding computation cost is:</p>
<p>$$ K\times K\times H\times W\times C_{in}. $$</p>
<p>The point-wise convolution is a simple 1x1 convolution (also known as network-in-network), which transfers the $C_{in}\times H\times W$ volume produced by the depth-wise operation to  a $C_{out}\times H\times W$ output volume. Since we have dealt with the input volume with a channel-by-channel strategy at first, so the purpose of point-wise operation is to combine the information of different channels and fuse them to new features. The point-wise operation costs</p>
<p>$$ 1\times 1\times C_{in}\times C_{out}\times H\times W = C_{in}\times C_{out}\times H\times W.$$</p>
<p>As a result, with the above decomposition, the total MACC is<br>
$$\begin{equation}\label{eq2} K\times K\times H\times W\times C_{in} + C_{in}\times C_{out}.<br>
\end{equation}$$</p>
<p>Compared with equation $\eqref{eq1}$, the reduction of computation is $\eqref{eq2}$/$\eqref{eq1}$ $=\frac{1}{C_{out}} + \frac{1}{K^2}$.</p>
<p>In addition, the number of parameters of the standard convolution filters is $K\times K\times C_{in}\times C_{out}$. With depth-wise and point-wise convolution, the number of parameters becomes $K\times K\times C_{in} + C_{in}\times C_{out} = C_{in}\times (K\times K + C_{out})$. In this way, both computation cost and model size can be considerably reduced. What’s more, this can be further done by applying the <em>Resolution Multipier</em> and <em>Width Pultipier</em>, which reduce the resolution of the input images and channels of all layers by a multipier coefficient.</p>
<p>If you are not clear, the following is the whole MobielNet v1 structure with all the bells and whistles.<br>
<img src="mobilenetv1_2.png"></p>
<p>The structure was drawn according to the code in <a href="https://github.com/marvis/pytorch-mobilenet">https://github.com/marvis/pytorch-mobilenet</a>, where filter in each row of the table takes the input with size written immediately in the same row, and therefore, outputs a volume with size written in the following row, and then, processed by the next filter. Finally, <code>BR</code> means Batch normalization and Relu layers after a certain filter.</p>
<p>What surprised me was that there is no residual module at all, what if we add some residuals or shortcuts like ResNet? Afterall, the author got his purpose and the accuracy on ImageNet classification task is comparable to the one using the standard convolution filters instead as well as other famous CNN models.<br>
<img src="mobilenetv1_3.png" width="500"></p>
<h2 id="MobileNet-v1-vs-SqueezeNet">MobileNet v1 vs. SqueezeNet</h2>
<p>First, let’s compare these two networks directly,</p>
<img src="3.1.png" width="500">
<p>where, <code>0.50 MobileNet-160</code> means halving the channels for all layers and setting the resolution of input images as $160\times 160$. We can see from the table that the only highlight of SqueezeNet is its model size. It is not ignorable that we also need the speed of computation when we embed our model into resource-restricted devices like Mobile phones. It’s hard to say that SqueezeNet is good enough when we see that its MACC is even more than AlexNet, with a large margin.</p>
<p>However, it’s worth thinking why SqueezeNet has so few parameters. Take a look at it basic unit (a fire module):</p>
<img src="3.2.png" width="450">
<p>The basic idea behind SqueezeNet comes from three principles. First, using 1x1 filters as possible as we can; Second, decreasing the number of input channels to 3x3 filters. The last pinciple is to downsample feature maps after the merging operation of residual blocks so that to keep more activations.</p>
<p>By stacking fire modules, we get a small model, while also having numerous computations.</p>
<h2 id="MobileNet-v1-vs-MobileNet-v2">MobileNet v1 vs. MobileNet v2</h2>
<p>Keep it in mind that MobileNet v1’s success attributes to using the <strong>depth-wise</strong> and <strong>point-wise</strong> convolutions. These two kinds of filters become the very basic tools for most of the following works focusing on network compression and speeding up, including MobileNet v2, ShuffleNet v1 and v2.</p>
<p>For the MobileNet v2, similar to the above illustration, let’s first take a look at its <a href="4.1.png" target="_blank">whole structure</a>. For analysis, we take part of it as the whole structure is stacked with similar components.</p>
<img src="4.2.png">
<p>In this illustration, the green unit means a residual block while the orange one means a normal block (without residual) with stride 2 to do downsampling. The main characteristic of MobileNet v2 architecture is for every unit or block, it first expands the number of channels by point-wise convolutions, then applies depth-wise convolutions with kernel size 3x3 on the expanded space, and finally projects back to low-channel feature spaces using point-wise convolutions again. For a block doesn’t having to downsample its input volume, an additional residual component is applied to enhance the performance. Another feature is, as illustrated in the above figure with a single <code>B</code> after each block which means Batch normalization only, it doesn’t use non-linearity at the output of blocks. Now, I have the following questions:</p>
<ol>
<li>When building a residual block, why connect the shortcut between two low-channel ends? Why not connect the “fat part” just like the original ResNet does?</li>
<li>Why it needs to be “fat” in the middle of block? Why not just keep it slim so that to further reduce its size and parameters? Why not apply ReLu at the end of block?</li>
<li>Comparing with ResNet, which applies ReLU on its “slim part” of each block, it seems like the two designing strategies (ResNet block and MobileNet v2 block) conflict with each other, why?</li>
</ol>
<p>OK, let’s try to answer these questions (if you have any different idea, please do not hesitate to contact me, the email can be found in my profile).</p>
<p>For question 1, there is a intuition when designing MobileNet v2: bottlenecks actually contains all the necessary informations. So it would not cause information loss if we do like that. On the other hand, connecting the “fat parts” is possible, but that also means we should connect two volumes produced by two depth-wise convolutions, sounds strange because we usually connect the outputs of normal convolutions (here a point-wise covolution is a normal 1x1 convolution), but nothing stops trying.</p>
<p>For question 2, we can find our answer from the analysis of ReLU.</p>
<img src="4.3.png" width="500">
<p>ReLu cause information collapse. However, the higher the dimension of the input, the less the degree information collapses. So the high dimension in the middle of block is to avoid information loss. And intuitively, more channels usually means more powerful representative features thus to enhance the discriminability of a model. According to this, it is reasonable not to apply ReLU at the “slim output” of the block.</p>
<p>We can use the same explanation to attack ResNet, which indeed use ReLU on the low-dimensional features. So why is it still so effective? This would attribute to its high dimensions of input and output ends of a ResNet block, which ensure its representative ability even with the ReLU layer in the bottleneck.</p>
<p>The design art of MobileNet v2 is to keep few number of channels for the input and output of each block, while doing more complicated feature extraction inside the block with enough channels. This ensures the extraction of effective and high-level features of the image while reduce the computation cost at the same time, because <strong>the main computation cost is from the 1x1 convolution filters</strong> (see the following figure).</p>
<img src="4.4.png" width="400">
<p>MobileNet v2 has even less parameters and MACCs than v1. This because MobileNet v1 takes more channels for 1x1 convolutions than v2, leading to much more MACCs. While MobileNet v2 smartly avoid giving many channels to 1x1 convolutions, and do feature extraction mainly via depth-wise convolutions.</p>
<h2 id="MobileNet-v2-vs-ShuffleNet-v1-vs-NasNet">MobileNet v2 vs. ShuffleNet v1 vs. NasNet</h2>
<img src="5.1.png" width="400">
<p>Above figure shows that a ShuffleNet v1(1.5) and a MobileNet V2 have the similar model size (3.4M params) and computation cost ($\approx 300$M MACCs), and furthermore, the similar classification accuracy. This means that ShuffleNet v1 is at the same level of MobileNet v2, the two are closely comparable. So, what does a ShuffleNet v1 look like? <a href="5.2.png" target="_blank">Click here</a></p>
<p>Again, we capture part of it to analyse.</p>
<span id="compress">
<img src="5.3.png">
</span>
<p>Since we realize that the main computation takes place at the 1x1 convolutions, which also accounts for main part of parameters. Unlike MobileNet v2 who solves the problem by reducing number of channels inputted to 1x1 convolutions, ShuffleNet v1 is more straightforward. Specifically, rather than only applying group convolution (for group convolution, see ResNeXt, depth-wise convolution can be regarded as an extreme case of group convolution) on 3x3 filters, it also applies group operation on 1x1 filters. Although it reduces computation cost and number of parameters effectively, it leads to a problem: different groups cannot communicate with each other, thus restrict the power of model.</p>
<p><code>Shuffle</code> in ShuffleNet v1 provides the solution of above problem by shuffling all the output channels of 1x1 group convolutions as a whole, so that enforce information communication among groups. And the most inspiring thing is the shuffle operation doesn’t take any additional parameters and computationally efficient.</p>
<img src="5.4.png" width="200">
<p>To further reduce model size and computation cost, ShuffleNet v1 also uses <code>BottleNeck</code>s as illustrated:</p>
<img src="5.5.png" width="600">
<p>As discussed above, MobileNet v2 and ShuffleNet v1 both focus on reducing computation cost on 1x1 convolutions, while there are still three more differences according to their structures.</p>
<ol>
<li>Difference on how to apply residual. For MobileNet v2, no residual is used when the shape of input volume and output volume of a block doesn’t match. For ShuffleNet v1, when the two doesn’t match, a <code>AveragePool + Concatenation</code> strategy is used to do shortcut connection.</li>
<li>According to the above <a href="5.2.png" target="_blank">diagram</a>, ShuffleNet v1 quickly downsamples the input image from 224x224 to 56x56, while <a href="4.1.png" target="_blank">MobileNet v2</a> only downsamples its input image to 112x112 in the first stages.</li>
<li>According to the logic of MobileNet v2, ReLU layers should apply on “fat layers” rather than bottleneck layers. While ShuffleNet (both v1 and v2) more or less does the opposite (e.g., ReLU after the <a href="#compress">Compress</a> operator, marked red in the figure). Why?</li>
</ol>
<p>Well, I think it’s worth trying and see what will happen if we take the ReLU away after the 3x3 convolutions in MobileNet v1 or MobileNet v2 (e.g., only connect the ReLu to the first 1x1 convolution layer of each block mobileNet v2). On the other hand, the reason why ShuffleNet v1 doesn’t connect a ReLU after the 3x3 convolution layers comes from the explanation in Xception, which thought that for shallow features (i.e., the 1-channel deep feature spaces of depth-wise convolutions), non-linearity becomes harmful, possibly due to a loss of information.</p>
<p><strong>NasNet</strong>, in which the word “Nas” is an abbreviation of <strong>Network architecture search</strong>, definitely is a more advanced technology to search for compact and efficient networks. The auto-search algorithms and other very recent research works (works in ICLR 2019, ICML 2019 and CVPR 2019) will be gone through in another post. Let’s proceed to ShuffleNet v2.</p>
<h2 id="ShuffleNet-v2-vs-All">ShuffleNet v2 vs. All</h2>
<p>The above methods are based on two principles, small model size and less computation cost. However, in practical applications, efforts taken on the above criterion doesn’t exactly bring a corresponding faster model in hardware equipments. There are some other factors we should take into account when designing an embeddable model for hardware devices – memory access cost (MAC) and battery consuming.</p>
<p>Based on the above findings, ShuffleNet v2 rethinks the previous compression models and proposes four useful designing guidelines.</p>
<blockquote>
<p>G1, Equal channel width minimizes MAC (this means letting number of input channels equal to that of output channels);<br>
G2, Excessive group convolution increase MAC (do not use or use less group convolutions);<br>
G3, Network fragmentation reduces degree of parallelism (small stacked convolutions with in blocks and branches in NasNet);<br>
G4, Element-wise operations are non-negligible (like ReLU and addition operations in residual block).</p>
</blockquote>
<p>As described in the original paper, ShuffleNet v1 violates G2 (group convolutions) and G1 (bottleneck blocks), MobileNet v2 violates G1 (inverted bottleneck structure) and G4 (ReLU on “thick” feature maps), and NasNet violates G3 (too many branches).</p>
<p>So the problem is:</p>
<blockquote>
<p>How to maintain a large number and equally wide channels with neither dense convolution nor too many groups?</p>
</blockquote>
<p>We mention that all the above guidelines have been proved by a series of validation experiments. Let’s draw the building blocks of ShuffleNet v2 here (actually I’ve also drawn a table for ShuffleNet v2 structure <a href="6.2.png" target="_blank">here</a>, but takes time to understand…)</p>
<img src="6.1.png" width="700">
<p>How does it solve the problem?</p>
<ul>
<li>First, the <code>channel split</code> divide the input channels into two parts, one of them keeps untouched, the other experiences a <strong>1x1 + DW3x3 + 1x1</strong> flowchart, here, the <strong>1x1</strong> doesn’t use group convolution. On one hand to follow <strong>G2</strong>, on the other hand, two branches indicates two groups.</li>
<li>Second, the two branches are merged by concatenation. By doing so, there is no add operations (follows <strong>G4</strong>), and all the ReLU and depth-wise convolutions only exist in half of all the input channels, which again follows <strong>G4</strong>.</li>
<li>Then, after concatenation, channel shuffling is applied to enforce branch communication. In addition, the <strong>Concat + Shuffle + Split</strong> pipeline can be merge into a single element-wise operation, which follows <strong>G4</strong>.</li>
<li>Similar to DenseNet, it takes the advantage of <code>feature reuse</code>.</li>
</ul>
<p>Under the same FLOPs, ShuffleNet v2 is superior than other models.</p>
<img src="6.3.png" width="700">
<h2 id="Conclusion">Conclusion</h2>
<p>We have analysed several classical network compression models, from which we can see that the main strategy to reduce model size and computation cost is using <strong>Depth-wise convolution</strong>, <strong>Group convolution</strong> and <strong>Point-wise convolution</strong>.</p>
<p>There are other interesting algorithms like network pruning, network quantization (e.g., binarize weiths and activations) and Network architecture search. They also lead to fast and small network models and will be discussed in the next post.</p>
<p><em>Note: Most of the figures are directly copied from the original paper.</em></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/abs/1704.04861">Howard, Andrew G., et al. “Mobilenets: Efficient convolutional neural networks for mobile vision applications.” arXiv preprint arXiv:1704.04861 (2017).</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/abs/1801.04381">Sandler, Mark, et al. “Mobilenetv2: Inverted residuals and linear bottlenecks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/abs/1602.07360">Iandola, Forrest N., et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size.” arXiv preprint arXiv:1602.07360 (2016).</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf">Zhang, Xiangyu, et al. “Shufflenet: An extremely efficient convolutional neural network for mobile devices.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf">Ma, Ningning, et al. “Shufflenet v2: Practical guidelines for efficient cnn architecture design.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf">Zoph, Barret, et al. “Learning transferable architectures for scalable image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Network compression</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-LM (1)</title>
    <url>/blogs/2024/12/26/Megatron-LM-1/</url>
    <content><![CDATA[<h2 id="Resources-about-distributed-training-with-Megatron-LM">Resources about distributed training with Megatron-LM</h2>
<p>Github: https://github.com/NVIDIA/Megatron-LM<br>
Document on NeMo: https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html</p>
<blockquote>
<blockquote>
<p>NeMo is a cloud-native generative AI framework built on top of Megatron-LM.</p>
</blockquote>
</blockquote>
<p>Overall view of Megatron-Core: https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html</p>
<blockquote>
<blockquote>
<p>Official APIs with formal product support…</p>
</blockquote>
</blockquote>
<p>Megatron-LM are basically based on the following three papers. Let’s do some notes on them.</p>
<h2 id="Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</h2>
<span id="more"></span>
<p>paper (2020, arxiv): <a href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></p>
<h3 id="Contributions">Contributions</h3>
<ol>
<li>Put large transformer models into different GPUs (with <code>tensor model parallelism</code>) to solve the problem that a single GPU cannot fit the whole model.</li>
<li>No need to design custom C++ code, compatible with existing Pytorch transformer implementations.</li>
<li>Able to train a GPT-2 with 8.3 billion parameters and a BERT with 3.9 billion parameters.<br>
<img src="/blogs/joplin_resources/fa846f2d8740035590fed9a5b6e21774.png" alt=""><br>
On the above figure, model parallel means using tensor model parallelism methods proposed in this paper. Evaluation is based on weak scaling.</li>
</ol>
<h3 id="Methods">Methods</h3>
<p><img src="/blogs/joplin_resources/ea97528d7e5fcdf8c5320fd26d4ef537.png" alt=""></p>
<p>The tensor model parallelism can be described by the above illustrations. Generally it is designed for equally partitioning transformer blocks (MLP and self-attention layers) into different parts which are stored in corresponding GPUs. Above figure uses 2 GPUs, it means both MLP and Self-Attention layer are segmented equally with two parts, with each part put in a GPU during training.</p>
<p>For MLP, we have the following equations:<br>
$$ Y= \text{GeLU}(XA) $$<br>
$$ Z = \text{Dropout}(YB) $$<br>
where X and Y are activations, A and B are parameter matrices. A is split along columns such that the GeLU nonlinear function can be put in individual GPUs separately, leading to $Y_1$ and $Y_2$. Then B is split along rows giving $Z_1=Y_1B_1$ and $Z_2=Y_2B_2$. Before dropout, we should have $Z=Z_1 + Z_2$, therefore, we use “all-reduce” operator to calculate the sum from different GPUs and distribute the result back to all GPUs, then dropout operator is executed in each GPU that outputs $Z$ (here I guess each GPU should share the same dropout mask).</p>
<p>In this way, the “f” function is actually a non-operation (or Identity function) and “g” is an all-reduce function in the forward pass. In the backward pass, “g” becomes Identity function and “f” becomes an all-reduce function. These two functions are the so-called conjugate functions.</p>
<p>For Self-Attention layer, we make use of the multi-head attention mechanism to do tensor model parallelism. X, again, is shared in all GPUs, while each GPU have its separate sets of attention heads where the K, Q, V are generated with its own linear projection matrices. Similarly, B is split in rows and all-reduce is applied before dropout.</p>
<p>Overall, after applying such tensor model parallelism, for each transformer layer (consisting of a attention layer and a MLP layer), there are 4 total communication operations in the forward and backward pass of a single model parallel transformer layer, i.e., four all-reduce operations involved in forward and backward passes.<br>
<img src="/blogs/joplin_resources/c49cabcf330bbd76d7f63d0cf30a0da4.png" alt=""></p>
<h4 id="Other-notes">Other notes</h4>
<ol>
<li>
<p>For output embedding $E_{H\times v}$ which transforms the hidden size H to vocabulary size v, we split $E$ along columns to $E_1, E_2$ and multiply with the output of the last transformer layer to get $[Y_1, Y_2] = [XE_1, XE_2]$, then instead of using all-gather to gather $Y_1, Y_2$ to $Y=[Y_1, Y_2]$ and distribute it to each GPU followed by cross-entropy loss (this may cause the all-gather operation to communicate $b\times s\times v$ elements in $Y$ where b is batch size and s is sequence length), they  fuse the output of $[Y_1, Y_2]$ with the cross entropy loss to reduce the dimension to $b\times s$. (Though here I don’t know how they fuse that :&lt;).</p>
</li>
<li>
<p>For communications between GPUs, they use NVSwitch with 300GB/sec bandwidth for intra-server and 8 InfiniBand adapters per server with 100GB/sec bandwidth for inter-server communications.</p>
</li>
</ol>
<h3 id="Scaling-evaluation-on-GPT-2">Scaling evaluation on GPT-2</h3>
<p><img src="/blogs/joplin_resources/798943eea9a9e4d297607659287469f7.png" alt=""><br>
Here, 100% is for the baseline regarding the training throughput. Other percentages are relative to the baseline.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-LM (2)</title>
    <url>/blogs/2024/12/26/Megatron-LM-2/</url>
    <content><![CDATA[<h2 id="Efficient-Large-Scale-Language-Model-Training-on-GPU-Clusters-Using-Megatron-LM">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h2>
<span id="more"></span>
<p>paper (2021 arxiv): <a href="https://arxiv.org/abs/2104.04473">https://arxiv.org/abs/2104.04473</a></p>
<h3 id="Contributions">Contributions</h3>
<p>Based on Megatron-LM, the authors further adopt pipeline model parallelism via the proposed interleaved 1F1B pipeline schedules to scale the LLM training to thousands of GPUs, because of the dramatic increase in model sizes.</p>
<p><img src="/blogs/joplin_resources/5bc748b36099a88976c17f640972d76b.png" alt=""></p>
<h3 id="Methods">Methods</h3>
<p><img src="/blogs/joplin_resources/98465e80b56627ea161b155d620a0b07.png" alt=""></p>
<p>Basically, it combines tensor parallelism (proposed in Megatron-LM) and pipeline parallelism for model parallelism. Assume tensor-parallel size is $t$, pipeline-parallel size is $p$ (also said as the number of pipeline stages), and data-parallel size is $d$, then the total number of GPUs is $ptd$.</p>
<p>First, let's get familiar with some concepts:</p>
<h4 id="GPipe-pipeline-schedule">GPipe pipeline schedule</h4>
<p><img src="/blogs/joplin_resources/6575e9290eb6cf6aa228877c5239e812.png" alt=""></p>
<blockquote>
<p>The grey area represents <span style="color: red;">pipeline bubble</span> where devices are idle. <span style="color: red;">Pipeline flush</span>, I guess, is the end point of the backward pass at each iteration.</p>
</blockquote>
<p>Here, the number of microbatches in batch is $m$, and assume the time to execute a single microbatch’s forward and backward pass as $t_f$ and $t_b$. Then the total amount of time spent in the pipeline bubble is $t_{pb} = (p-1)\cdot (t_f + t_b)$, and the ideal processing time for the batch is $t_{id} = m\cdot (t_f + t_b)$. Therefore, the fraction of ideal computation time spent in the pipeline bubble (or called the <span style="color: red;">bubble time fraction</span>) is:</p>
<p>$$<br>
\frac{t_{pb}}{t_{id}} = \frac{p - 1}{m}.<br>
$$</p>
<p>The bubble time fraction should be as small as possible. The naive solution is to make $m \gg p$, however, this needs each device to store all the m microbatches’ activations for the gradient calculation in the backward pass, having a high memory footprint. In other words, the number of in-flight microbatches equal to the total number of microbatches $m$, so we have:</p>
<h4 id="PipeDream-Flush-schedule-upper-part-of-the-following-figure">PipeDream-Flush schedule (upper part of the following figure)</h4>
<p><img src="/blogs/joplin_resources/6e4ffba59ca4f57b0bbc132fd516ee3b.png" alt=""></p>
<p>In this schedule, there is a warmup phase for each device for the forward pass. After the warmup, the device goes with a one forward pass followed by one backward pass, which is the so-called <span style="color: red;">1F1B</span> schedule. In this way, the number of in-flight microbatches reduces to $p$ in maximum, while the bubble fraction time is the same. Therefore, PipeDream-Flush can be much more memory-efficient when $p \ll m$.</p>
<p>To reduce the bubble fraction time and keep the schedule memory efficient, the authors propose:</p>
<h4 id="Interleaved-1F1B-pipeline-schedule-lower-part-of-the-above-figure">Interleaved 1F1B pipeline schedule (lower part of the above figure)</h4>
<p>Briefly, each device can perform computation for multiple subsets of layers (or a <span style="color: red;">model chunk</span>), for example, device 1 has layers 1, 2, 9, 10, device 2 has layers 3, 4, 11, 12, and so on. Then just like the 1F1B schedule, they do an interleaved 1F1B schedule.</p>
<ul>
<li>Property 1: this needs the number of microbatches $m$ to be an integer multiple of $p$;</li>
<li>Property 2: this reduce the pipeline bubble time to $(p-1)\cdot(t_f+t_b)/v$ where $v$ is the number of model chunks in each stage. Then the bubble time fraction reduces to $(p-1)/(m\cdot v)$.</li>
<li>Property 3 (drawback): this introduces extra communication with the increase of $v$.</li>
</ul>
<h3 id="Higher-level-experimental-and-analytical-conclusion">Higher-level experimental and analytical conclusion</h3>
<p>The actual throughput for each device is affected by all the hyperparameters $p, t, d, B, b$ etc., where $B$ is the global batch size and $b$ is the microbatch size due to the communication overhead between devices. There are three takeaways</p>
<ol>
<li>
<p>When using $g$-GPU servers, the tensor model parallelism should generally be set up to $g$. Based on that, pipeline model parallelism can be used across servers.</p>
</li>
<li>
<p>When combine data and model parallelism, for model parallelism, a total number of $t\cdot p$ GPUs should be used to fit the model memory, then data parallelism is used to scale up training.</p>
</li>
<li>
<p>The optimal $b$ depends on the characteristics and throughput of the model, $p$, $d$ and $B$.</p>
</li>
</ol>
<h3 id="Communication-optimization">Communication optimization</h3>
<p><img src="/blogs/joplin_resources/8650bd09481bc21b74e8e18f303e065f.png" alt=""></p>
<p>Simply, shown in the above figure, assume we have $t=2$, when send and receive between two consecutive pipeline stage (i.e., two servers according to conclusion 1), the naive way is to send the tensor on each GPU on the previous pipeline stage (server) to the second stage, where each pair of GPUs on the sender and receiver communicate with the exact same set of tensor.  Instead, we can first divide (scatter) the tensor to be sent into $t$ parts equally and each GPU on the sender server send its part to the GPU on the receiver server, then use all-gather operator to gather the tensor. We call this scatter/gather optimization that reduce the communication to $1/t$.</p>
<h3 id="General-accelerator-agnostic-ideas">General accelerator-agnostic ideas</h3>
<ol>
<li>Smartly partitioning the model training graph to minimize the amount of communication while still keeping device active (they are saying the interleaved 1F1B pipeline schedule).</li>
<li>Minimizing the number of memory bound kernels with operator fusion and careful data layout (they might be saying the fusion of output embedding with the cross entropy loss function to reduce the communication overhead).</li>
<li>Other domain-specific optimizations (like the scatter-gather optimization).</li>
</ol>
<h3 id="Experiment-results">Experiment results</h3>
<p><img src="/blogs/joplin_resources/e6acb1a769d391f7aaa6b4d449df3a7f.png" alt=""></p>
<p><img src="/blogs/joplin_resources/c63f7301c8ccbcbdd2535c492b8ecd36.png" alt=""></p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-LM (3)</title>
    <url>/blogs/2024/12/26/Megatron-LM-3/</url>
    <content><![CDATA[<h2 id="Reducing-Activation-Recomputation-in-Large-Transformer-Models">Reducing Activation Recomputation in Large Transformer Models</h2>
<span id="more"></span>
<p>paper (2022 arxiv): https://arxiv.org/abs/2205.05198</p>
<h3 id="Contributions">Contributions</h3>
<p>Briefly, the authors based on tensor and pipeline model parallelism, find that the previous parallelism cannot reduce the memory needed for activations while maintaining high device utilization. They propose <span style="color: red;">sequence parallelism</span> and <span style="color: red;">selective activation recomputation</span>.</p>
<h3 id="Methods">Methods</h3>
<h4 id="Sequence-parallelism">Sequence parallelism</h4>
<p>Let’s first analyze the activation memory during training for a single transformer layer without gradient checkpointing (or activation recomputation).</p>
<p><img src="/blogs/joplin_resources/3d18b37149176ab5f6bbaa98853201e7.png" alt=""><br>
<img src="/blogs/joplin_resources/bc097eaac647dcdf091c8480115856aa.png" alt=""><br>
Above is the transformer structure with layer normalizations rearranged in Megatron-LM. Supposing the network and activations are stored in a 16-bit floating point format (2 bytes for each element) and the dropout masks only need 1 byte to store. The activation memory (in bytes) needed for each component is</p>
<ul>
<li>LayerNorm: $4sbh$</li>
<li>Self Attention as follows:<br>
<img src="/blogs/joplin_resources/0ef95fd501438d6f6458d1d8cfab0f92.png" alt=""><br>
Q, K, V: $6sbh$<br>
QK^T + Softmax output: $2s^2b$<br>
Dropout mask: $s^2b$<br>
Dropout output: $2s^2b$<br>
Attention with V: $2sbh$<br>
In total: $8sbh + 5s^2b$</li>
<li>Linear ($h\rightarrow h, h\rightarrow 4h, 4h\rightarrow h$): $2sbh + 8sbh + 2sbh = 12sbh$</li>
<li>Dropout  (masks + output): $1sbh + 2sbh + 1sbh + 2sbh = 6sbh$</li>
<li>GeLU: $8sbh$</li>
</ul>
<p>In total: $34sbh + 5s^2b = sbh(34 + 5\frac{as}{h})$.</p>
<p><img src="/blogs/joplin_resources/198919038772856844e278de52ef0cf8.png" alt=""></p>
<p>However, when applying tensor parallelism (above figure), the output of LayerNorm ($4sbh$), output of dropout layers and dropout masks ($4sbh + 2sbh=6sbh$) are stored in all GPUs. Therefore, $10sbh$ are not parallelized. Therefore, the activations memory per layer is:<br>
$$sbh(10+\frac{24}{t}+5\frac{as}{ht})$$</p>
<p>The authors then propose sequence parallelism to also parallelize these $10sbh$ tensors along their sequency dimension:<br>
<img src="/blogs/joplin_resources/ab9adeb4b25f20c6d5ab41737ddd204b.png" alt=""></p>
<p>To do that, for example, apply sequence parallelism on the two side of MLP tensor parallelism, the method is illustrated as follows:<br>
<img src="/blogs/joplin_resources/a480dd0135ebd2f433d33d1c2fd5bfa7.png" alt=""></p>
<p>Particularly, before the “$g$” function, $Y_1^s, Y_2^s$ are the outputs from the previous sequence parallel stage which are divided along the sequence dimension, therefore, $g$ function uses “all-gather” operator to concatenate them and distribute the result to each tensor parallel device. The outputs of the tensor parallel stage should be processed with “all-reduce” that adds together the outputs and then distributed to each device for the dropout, instead, $\bar g$ uses “reduce-scatter” to scatter (or divide) the reduced results into different segments along the sequence dimension again, each sequence parallel GPU takes one segment for the dropout, followed by another tensor parallel stage.</p>
<p>In this way, all the activations are paralleled. There is no extra communications. Before sequence parallelism, for one forward and backward pass, it needs 4 all-reduce operations. Now, it needs 4 all-gather and 4 reduce-scatter operations, which have the same communication overhead.</p>
<h5 id="Other-notes">Other notes</h5>
<p>The authors also discussed the activations on input and output embeddings, which are negligible compared with the transformer layers.</p>
<h4 id="Selective-Activation-Recomputation">Selective Activation Recomputation</h4>
<p>Generally, we don’t store the softmax output, dropout mask and dropout output which take large amount of memory (the $5\frac{as}{ht}$ term). When doing backward pass, we recompute them based on the stored Q, K, V. However, the calculation of these activations is compute-efficient, so it makes more sense to recompute them.</p>
<p><img src="/blogs/joplin_resources/c544c6ee879141464cc291dbdaf48d0a.png" alt=""></p>
<p>In Table 4, the observation is that the introduction of selective activation recomputation only slightly affects the training speed, while the introduction of sequence parallelism reduce the overhead and speedup training. When two techniques combined, the speed and overhead just slightly affected, but the activation memory per device is significantly reduced (all tensors to $1/t$), which is important for scaling to large models.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Quick read: methods of network compression in 2019</title>
    <url>/blogs/2019/06/22/Quick-read-methods-of-network-compression-in-2019/</url>
    <content><![CDATA[<h2 id="Overview">Overview</h2>
<p>Let’s quickly go through the new models related to network compression published at <em>CVPR 2019</em>, <em>ICLR 2019</em> and <em>ICML 2019</em>. Some works needs to be read and understood more carefully.</p>
<span id="more"></span>
<h2 id="CVPR-2019">CVPR 2019</h2>
<p>CVPR is more kind of tending to solve problems in practical applications, while ICLR and ICML are more close to theoretical explanations.</p>
<h3 id="1-Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression">1. Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></h3>
<ul>
<li>
<p>Institutes: Xiamen University, Peng Cheng Laboratory (Shenzhen, China), Beihang University, Huawei Noahs Ark Lab, University of Buffalo and BestImage of Tencent Technology (Shanghai)</p>
</li>
<li>
<p>Notes</p>
</li>
</ul>
<ol>
<li>Investigate CNN compression from a novel interpretable perspective. Discover that importance of feature maps depend on sparsity and richness (using the proposed Kernel sparsity and Entropy metric);</li>
</ol>
<img src="2.1.png" width="400">
<ol start="2">
<li>Pruning in a feature-agnostic way, so that all layers can simultaneously be handled in parallel.</li>
<li>Using Kernel Clustering to replace the common kernel pruning methods.</li>
</ol>
<ul>
<li>Results<br>
ResNet-50 4.7x FLOPs, 2.9x Size and a reduction of 0.35% Top-5 accuracy on ImageNet.</li>
</ul>
<h3 id="2-Towards-Optimal-Structured-CNN-Pruning-via-Generative-Adversarial-Learning">2. Towards Optimal Structured CNN Pruning via Generative Adversarial Learning<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></h3>
<ul>
<li>Institutes: Xiamen University, Beihang University, UCAS (China), BestImage of Tecent Technology (Shanghai), University of Buffalo (the same group as above)</li>
<li>Notes</li>
</ul>
<ol>
<li>Using GAN to guide filter pruning. Specifically, the <code>Generator</code> is used to generate pruned network, the <code>Discriminator</code> is used to judge whether the output is from the original network or the pruned network with the Objective function based on L<sub>1</sub>-regularization.</li>
<li>Label free due to no need of label information.</li>
<li>Using a soft mask to build a generator.<br>
<img src="2.2.png" width="400"></li>
</ol>
<ul>
<li>Results<br>
ResNet-50 3.7x speedup and a reduction of 3.75% Top-5 accuracy on ImageNet. Not as good as the above one.</li>
</ul>
<h3 id="3-RePr-Improved-Training-of-Convolutional-Filters">3. RePr: Improved Training of Convolutional Filters<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></h3>
<ul>
<li>Institutes: Brandeis University, Microsoft Research</li>
<li>Notes</li>
</ul>
<ol>
<li>They discover that no matter the size of network, even those small under-parameterized networks, the network would always tend to learn redundant filters, which suggests that filter redundancy is not solely a result of over-parameterization, but is also due to ineffective training;</li>
<li>So the method of the work is to first train a network with standard training, then select a subset of the model’s filters to be temporarily dropped, continue training. After that, reintroduce the previously dropped filters which are initialized with new weights and train with standard training again. Do this several times, the performance would be improved than common standard training.</li>
</ol>
<img src="2.3.png" width="400">
<ol start="3">
<li>
<p>Therefore, the work also proposes a criterion to do filter selection (to select less useful filters).</p>
</li>
<li>
<p>I think it’s like dropout, to some extent the strategy proposed introduce a form of regularization to gain more generality. But my doubt is why giving up less useful filters and then reintroducing helps improving training? Perhaps when we reintroduce the filters and initialize them with new weights, it can increase the capacity of the subset which has already been trained. The new weights give a new opportunity to train a better model, because we gave up them when they were not useful. From this perspective, the proposed algorithm also seeks for better solution by initializing the reintroduced weights to make sure they are orthogonal to their value before being dropped and the current value of non-pruned filters, thus ensuring small redundancy.</p>
</li>
</ol>
<h3 id="4-Fully-Learnable-Group-Convolution-for-Acceleration-of-Deep-Neural-Networks-FLGC">4. Fully Learnable Group Convolution for Acceleration of Deep Neural Networks<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> (FLGC)</h3>
<ul>
<li>Institutes: CAS, UCAS</li>
<li>Notes</li>
</ul>
<ol>
<li>The <em>Introduction</em> section does a really good recall and conclusion in the network compression literature. It’s worth reading while the recall seems has nothing to do with the main algorithms proposed.</li>
<li>The main point of the paper is to propose a new strategy of <code>Group Convolution</code>, which can be seen as an improvement of <strong>ShuffleNet</strong>. The difference is, for ShuffleNet, only the <em>filters</em> are not fixed (input channels connected to each filter are changed through channel shuffle, so it also means the filter is changed while the input channels are fixed) during group convolution, while for this paper, both <em>input channels</em> and <em>filters</em> are not fixed and each filter can connect to different number of input channels.<br>
<img src="2.4.png" width="400"></li>
</ol>
<ul>
<li>Results<br>
<img src="2.5.png" width="400"></li>
</ul>
<h3 id="5-A-Main-Subsidiary-Network-Framework-for-Simplifing-Binary-Networks">5. A Main/Subsidiary Network Framework for Simplifing Binary Networks<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></h3>
<ul>
<li>Institutes: Zhejiang University, Harvard University, UC (University of California, San Diego), UESTC (China)</li>
<li>Notes</li>
</ul>
<ol>
<li>The authors prove that even for <code>Binary Network</code>, there exits redundancy.</li>
<li>So they <strong>prune Binary Network directly</strong>.</li>
</ol>
<ul>
<li>Results<br>
For binary ResNet-18 on ImageNet, the authors use 78.6% filters but can achieve slightly better test error 49.87% (50.02%-0.15%) than the original model.</li>
</ul>
<h3 id="6-Binary-Ensemble-Neural-Network-More-Bits-per-Network-or-More-Networks-per-Bit">6. Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></h3>
<ul>
<li>Institutes: UC San Diego, Harvard University (the same group as above)</li>
<li>Notes</li>
</ul>
<ol>
<li>They prove why the Binary network suffer from sever accuracy degradation, especially when the activations are also binarized, through extensive experiments on representation power, bias, variance, stability and robustness, and think that the degradation are not likely to be resolved by solely improving the optimization techniques.</li>
<li>Error of BNNs are predominantly caused by <strong>Intrinsic Instability</strong> and <strong>Non-robustness</strong>. Therefore, they propose a <code>Binary Ensemble Neural Network</code> (BENN) to boost performance.</li>
<li>BENN is faster and more robust than the state-of-art binary networks and sometimes even more accurate than the full-precision floating number of network.</li>
</ol>
<ul>
<li>Results<br>
<img src="2.6.png" width="400"></li>
</ul>
<h3 id="7-ESPNet-v2-A-light-weight-Power-Efficient-and-General-Purpose-Convolutional-Neural-Network">7. ESPNet v2: A light-weight, Power Efficient, and General Purpose Convolutional Neural Network<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></h3>
<ul>
<li>
<p>Institutes: University of Washington, Allen Institute for AI, XNOR.AI</p>
</li>
<li>
<p>Notes</p>
</li>
</ul>
<ol>
<li>Group point-wise and depth-wise dilated separable convolutions</li>
<li>Based on ESPNet<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> and better than that.</li>
</ol>
<h3 id="8-Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-FPGM">8. Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> (FPGM)</h3>
<ul>
<li>Institutes: University of Technology Sydney, JD.com, CETC, Huawei, Baidu Research</li>
<li>Notes</li>
</ul>
<ol>
<li>The norm-based criterion utilized in previous works lead to limitations due to failure of two requirements: Low deviation and small minimum norm.</li>
<li>Propose FPGM to prune filters regardless of the two requirements, by pruning replaceable filters containing redundant information.</li>
<li>The theoretic basement may come from <em>Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. In ICLR, 2018.</em><sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></li>
</ol>
<h3 id="9-MnasNet-Platform-Aware-Neural-Architecture-Search-for-Mobile">9. MnasNet: Platform-Aware Neural Architecture Search for Mobile<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></h3>
<ul>
<li>Institutes: Google Brain, Google Inc</li>
<li>Notes</li>
</ul>
<ol>
<li>Using Architecture search to find fast and high-performance CNN model.</li>
<li>Unlike previous models using FLOPs which are often inaccurate to evaluate model’s latency, they directly measure the read-world latency by executing the model on real mobile devices.<br>
<img src="2.7.png" width="400"></li>
<li>Accomplish two trade-offs:
<ul>
<li>Accuracy &amp; Inference latency: Formulate the design problem as a multi-objective optimization problem considering the two things.</li>
<li>Search space &amp; Layer diversity: Propose a novel factorized hierarchical search space (where each block in the stacked structure can be different, while layers in each block should be with the same structure)</li>
</ul>
</li>
</ol>
<ul>
<li>Results (On ImageNet)<br>
<img src="2.8.png" width="400"></li>
</ul>
<h3 id="10-HAQ-Hardware-Aware-Automated-Quantization-with-Mixed-Precision">10. HAQ: Hardware-Aware Automated Quantization with Mixed Precision<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></h3>
<ul>
<li>Institutes: MIT (Song Han)</li>
<li>Notes</li>
</ul>
<ol>
<li>Conventional quantization methods use the same number of bits for all layers, but as different layer have different redundancy and arithmetic behaviours (computation bounded or memory bounded), this strategy is sub-optimal and it’s necessary to use mixed precision for different layers.</li>
<li>Because there are numerous possibilities of design polices (which determine the bitwidth of both weights and activations for each layer), the authors introduce the RL (Reinforcement Learning) agent to automatically determine the quantization policy, and take the hardware accelerator’s feedback in the design loop. So it is <em>Hardware-Aware</em> by considering latency, energy and storage on the target hardware directly instead of relying on proxy signals like FLOPs and model size.</li>
</ol>
<ul>
<li>Results<br>
Reduce latency by 1.4~1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with fixed bitwidth (8-bit) quantization.</li>
</ul>
<h2 id="ICLR-2019">ICLR 2019</h2>
<h3 id="1-The-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks">1. The lottery ticket hypothesis: finding sparse, trainable neural networks<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></h3>
<ul>
<li>Institutes: MIT CSAIL</li>
<li>Notes</li>
</ul>
<ol>
<li>Find winning tickets (subnetworks) that then trained in isolation, they can reach test accuracy comparable to the original network.</li>
<li>Worth reading and understanding more deeply.</li>
</ol>
<h3 id="2-An-empirical-study-of-binary-Neural-Networks’-optimization">2. An empirical study of binary Neural Networks’ optimization<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup></h3>
<ul>
<li>Institutes: University of Oxford</li>
<li>Notes</li>
</ul>
<ol>
<li>The training process with Straight-Through-Estimator (STE) is not well-founded due to the discrepancy between the evaluated function in the forward path and the weight updates in the back-propagation, updates which do not correspond to gradients of the forward path.</li>
<li>Normally training a BNN needs many ad-hoc techniques (STE, optimizer, etc). These are well analyzed through the corresponding experiments in this paper and understood whether they are necessary, so that better training process is guided for BNN.</li>
</ol>
<h3 id="3-Rethinking-the-value-of-Network-pruning">3. Rethinking the value of Network pruning<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup></h3>
<ul>
<li>Institutes: UC Berkeley, Tsinghua University</li>
<li>Notes</li>
</ul>
<ol>
<li>An excellent work reconsidering the traditional way of network pruning (3-stage: Training-&gt;Pruning-&gt;Fine-tuning). Under this rethinking, many previous work seems not to be persuasive again due to the lack of theoretical support.</li>
<li>What’s that? Previous work thinks that <em>A model with fewer filters can not be trained from scratch to achieve the performance of a large model that has been pruned to be roughly the same size</em>. Now, this paper points out that <strong>it actually can</strong>. The contradiction behind this might be explained by less carefully chosen hyper-parameters, data augmentation schemes and unfair computation budget for evaluating baseline approaches.</li>
<li>It suggests that the value of automatic structured pruning algorithms sometimes lie in identifying efficient structures and performing implicit architecture search, rather than selecting “important” weights.</li>
</ol>
<h3 id="4-ProxylessNAS-Direct-Neural-Architecture-Search-on-Target-Task-and-Hardware">4. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup></h3>
<ul>
<li>Institutes: MIT (Song Han)</li>
<li>Notes</li>
</ul>
<ol>
<li>What is Proxy-based NAS (Network Architecture Search)? NAS utilizing proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs.</li>
<li>Why previous works are proxy-based? Because the prohibitive computational demand of conventional NAS algorithms makes it difficult to directly search the architecture on large-scale tasks. On the other hand, differentiable NAS can reduce GPU hours but also increase CPU memory consumption.</li>
<li>The drawback of proxy-based NAS: They are not guaranteed to be optimal on the target task.</li>
<li>How to solve the difficulties when using proxyless (directly optimizes neural network architectures on target task and hardware) NAS?<br>
<img src="3.1.png" width="400">
<ul>
<li>To reduce GPU hours, the authors first directly train on an over-parameterized network that contains all candidates and gradually prune redundant paths.</li>
<li>To reduce GPU memory consumption, the authors binarize network parameters, and train them via a gradient-based approach based on BinaryConnect<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup></li>
<li>To handle on non-differentiable hardware objective (e.g., latency), the authors model network latency as a continuous function and optimize it as regularization loss.</li>
</ul>
</li>
</ol>
<ul>
<li>Results<br>
With only 200 GPU hours, got same top-1 accuracy as <code>MobileNet v2 1.4</code>, while 1.8x faster.</li>
</ul>
<h3 id="5-Defensive-Quantization-When-Efficiency-meets-Robustness">5. Defensive Quantization: When Efficiency meets Robustness<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup></h3>
<ul>
<li>Institutes: MIT (Song Han)</li>
<li>Notes</li>
</ul>
<ol>
<li>It is observed that the quantized model is more vulnerable to adversarial attacks (which consist of subtle perturbations on the input image to fool the network to make wrong decisions).</li>
<li>The above fact is counter-intuitive because small perturbations should be denoised with low-bit representations. They analyzed that this issue is caused by the fact that error of one layer can be amplified significantly when passing through deep neural network.</li>
<li>They find that when the magnitude of the noise is small, activation quantization is capable of reducing it while fails when the noise is greater than a certain threshold. Based on this, they propose Defensive Quantization (DQ) to control the Lipschitz constant of the network so that noise is kept within a small magnitude for all layers.</li>
<li>DQ can also make quantization itself easier thanks to the constrained dynamic range.</li>
</ol>
<h2 id="ICML-2019">ICML 2019</h2>
<h3 id="1-Collaborative-Channel-Pruning-for-Deep-Networks">1. Collaborative Channel Pruning for Deep Networks<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup></h3>
<ul>
<li>Institutes: Tencent AI Lab, CAS, University of Texas at Arlington</li>
<li>Notes</li>
</ul>
<ol>
<li>OK, it unluckily can be categorized to the 3-stage network pruning methods discussed above.</li>
<li>Investigate how the inter-channel relationship can be utilized to guide pruning.</li>
</ol>
<h3 id="2-EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Network">2. EfficientNet: Rethinking Model Scaling for Convolutional Neural Network<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup></h3>
<ul>
<li>Institutes: [Google Research, Brain Team, Mountain View, CA]</li>
<li>Notes:</li>
</ul>
<ol>
<li>It is critical to balance all dimensions of network width/depth/resolution. Therefore, they scale the three dimensions simultaneously.<br>
The Intuition behind this is that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more find-grained patterns on the bigger image.</li>
<li>Architecture search + Scaling --&gt; EfficientNet, much better than the state-of-art.<br>
<img src="4.1.png" width="400"></li>
<li>There is also a similar network search paper: MNasNet, CVPR 2019</li>
</ol>
<h2 id="Others-in-2019">Others in 2019</h2>
<h3 id="1-Searching-for-MobileNetV3">1. Searching for MobileNetV3<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup></h3>
<ul>
<li>Institutes: Google AI, Google Brain</li>
<li>Notes:<br>
Network architecture search + Network design.</li>
<li>Results (ImageNet):<br>
<img src="5.1.png" width="400"></li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Exploiting_Kernel_Sparsity_and_Entropy_for_Interpretable_CNN_Compression_CVPR_2019_paper.pdf">Li, Yuchao, et al. “Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/abs/1903.09291">Lin, Shaohui, et al. “Towards Optimal Structured CNN Pruning via Generative Adversarial Learning.” arXiv preprint arXiv:1903.09291 (2019).</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Prakash_RePr_Improved_Training_of_Convolutional_Filters_CVPR_2019_paper.pdf">Prakash, Aaditya, et al. “RePr: Improved Training of Convolutional Filters.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://arxiv.org/abs/1904.00346">Wang, Xijun, et al. “Fully Learnable Group Convolution for Acceleration of Deep Neural Networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/abs/1812.04210">Xu, Yinghao, et al. “A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://arxiv.org/abs/1806.07550">Zhu, Shilin, Xin Dong, and Hao Su. “Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/abs/1811.11431">Mehta, Sachin, et al. “ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network.” arXiv preprint arXiv:1811.11431 (2018).</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://arxiv.org/abs/1804.00015">Watanabe, Shinji, et al. “Espnet: End-to-end speech processing toolkit.” arXiv preprint arXiv:1804.00015 (2018).</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://arxiv.org/abs/1811.00250">He, Yang, et al. “Filter pruning via geometric median for deep convolutional neural networks acceleration.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://arxiv.org/abs/1802.00124">Ye, Jianbo, et al. “Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers.” arXiv preprint arXiv:1802.00124 (2018).</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://arxiv.org/abs/1807.11626">Tan, Mingxing, et al. “Mnasnet: Platform-aware neural architecture search for mobile.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://arxiv.org/abs/1811.08886">Wang, Kuan, et al. “HAQ: Hardware-Aware Automated Quantization with Mixed Precision.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/abs/1803.03635">Frankle, Jonathan, and Michael Carbin. “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” arXiv preprint arXiv:1803.03635 (2018).</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://openreview.net/pdf?id=rJfUCoR5KX">Milad Alizadeh and Javier Fernández-Marqués and Nicholas D. Lane and Yarin Gal. “An empirical study of binary Neural Networks’ optimization.” ICLR. 2019.</a> <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="https://arxiv.org/abs/1810.05270">Liu, Zhuang, et al. “Rethinking the value of network pruning.” ICLR. (2019).</a> <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p><a href="https://arxiv.org/abs/1812.00332">Cai, Han, Ligeng Zhu, and Song Han. “ProxylessNAS: Direct neural architecture search on target task and hardware.” arXiv preprint arXiv:1812.00332 (2018).</a> <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p><a href="http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf">Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. “Binaryconnect: Training deep neural networks with binary weights during propagations.” Advances in neural information processing systems. 2015.</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p><a href="https://arxiv.org/abs/1904.08444">Lin, Ji, Chuang Gan, and Song Han. “Defensive quantization: When efficiency meets robustness.” arXiv preprint arXiv:1904.08444 (2019).</a> <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="http://proceedings.mlr.press/v97/peng19c/peng19c.pdf">Hanyu Peng, Jiaxiang Wu, Shifeng Chen, Junzhou Huang ; Proceedings of the 36th International Conference on Machine Learning, PMLR 97:5113-5122, 2019.</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="http://proceedings.mlr.press/v97/tan19a/tan19a.pdf">Mingxing Tan, Quoc Le ; Proceedings of the 36th International Conference on Machine Learning, PMLR 97:6105-6114, 2019.</a> <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p><a href="https://arxiv.org/abs/1905.02244">Howard, Andrew, et al. “Searching for mobilenetv3.” arXiv preprint arXiv:1905.02244 (2019).</a> <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Network compression</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CLIP 相关工作1</title>
    <url>/blogs/2024/06/26/CLIP-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C1/</url>
    <content><![CDATA[<h2 id="1-Language-driven-Semantic-Segmentation">1. Language-driven Semantic Segmentation</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/535b77b874425a0a97a9b31c663b52bf.png" alt=""></p>
<p>First, using clip text encoder to get N tokens, each has C dimensional features. Use a pretrained image encoder (like ViT) to get H’ X W’ image tokens with each also having C dimensional features. Then by matrix multiplication, we get a HW X N matrix representing the correlations between each pixel and the text. The image encoder is then fine-tuned to maximize the correlation between the text embedding and the image pixel embedding of the ground-truth class of the pixel.</p>
<p>During testing, zero-shot can be done by also calculating the similarities between pixel embedding and each of the text embedding, just like CLIP.</p>
<h2 id="2-GroupViT-Semantic-Segmentations-Emerges-from-Text-Supervision">2. GroupViT: Semantic Segmentations Emerges from Text Supervision</h2>
<p><img src="/blogs/joplin_resources/9248ef8acd4a29ee844d358ea1ecd948.png" alt=""></p>
<p>Image is fed to patch tokenizer to get N tokens, along with them, there are G1 learnable tokens representing G1 semantic groups. After several layers of Transformer blocks, using grouping block to reduce the number of tokens from N + G1 to G1. Then there are another G2 learnable semantic groups that might have higher-level semantic meanings, repeating the process, to get G2 tokens in the final output. After a avg pooling and MLP, we get a single token that can be used to calculate the contrastive loss with the text embedding from clip. Negative pairs can be generated by using unmatched text. This is training.</p>
<p><img src="/blogs/joplin_resources/0c72d92fed60d21e0e199d23e7859081.png" alt=""></p>
<p>During testing, we use the final G2 tokens and compute their similarities to each class label text embedding. Use a threshold to extract those tokens, therefore the corresponding pixels in the original image as the segmented area.</p>
<h2 id="3-Open-vocabulary-Object-Detection-via-Vision-and-Language-Knowledge-Distillation">3. Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</h2>
<p><img src="/blogs/joplin_resources/b65089670121cca95fdb705ebe2645d8.png" alt=""></p>
<p>以上是要解决的问题。</p>
<p><img src="/blogs/joplin_resources/f48f4ea48059c7e9e9b0b0a2aa036b1a.png" alt=""></p>
<p>All these framworks are based on region proposals (e.g., from RPN, ROI align, etc.), then in ViLD-text, we get N region embeddings from the N proposals, then each proposal has a GT label (basic label), which can have a text embedding from clip. Additionally there is a background text embedding which is learnt. Then use the contrastive loss to train the region encoder.</p>
<p>In ViLD-image, because the dataset is huge, and generating N proposals from each image during training is not that feasible, so we pre-computed M proposals according to the whole dataest. Use clip image encoder to get M image embeddings, which can be a teacher to guide the training of the image encoder in the model, simply with L1 loss.</p>
<p>In ViLD, both knowledge distillation from clip image encoder and contrastive loss from clip text encoder are used, by simply feeding N + M proposals to the transformer and split them into N and M embeddings which are separately used to calculate the two losses.</p>
<p>During testing, novel labels can have corresponding regions in the image, using the same mechnism as in CLIP.</p>
<h2 id="4-Grounded-Language-Image-Pre-Training-GLIP">4. Grounded Language-Image Pre-Training (GLIP)</h2>
<p>Use image grouding dataset, which also has bounding box annotations, to train a image region encoder that miminize the contrastive loss between region embeddings and text embeddings, just like ViLD-text. Then it uses the trained model to generate bounding boxes (noisy lables) for more (24M) image-text pairs and use these generatel lables to train a larger model.</p>
<p>The large model GLIP-T uses Swin-L.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>CLIP</tag>
      </tags>
  </entry>
  <entry>
    <title>DPO - Direct Preference Optimization</title>
    <url>/blogs/2024/07/05/DPO-Direct-Preference-Optimization/</url>
    <content><![CDATA[<h2 id="Direct-Preference-Optimization-Your-Language-Model-is-Secretly-a-Reward-Model">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</h2>
<p>Tutorial https://www.youtube.com/watch?v=hvGa5Mba4c8<br>
Paper: https://arxiv.org/abs/2305.18290</p>
<span id="more"></span>
<h3 id="Highlight">Highlight</h3>
<p>It solve the RLHF problem which aims to align the output of LLMs to the uer desires, for example, to give human-prefered outputs that are more polite, friendly, no descrimination, etc. but without using reinforcement learning, but the common SGD.</p>
<h3 id="How">How?</h3>
<p>Recall in RLHF, we train a reward model (e.g., Bradley Terry model) $r^*$ such that for a human-prefered answer $y_1$ and a unprefered answer $y_2$ for a given prompt $x$, the probability that $y_1$ ranks before $y_2$<br>
<img src="/blogs/joplin_resources/85da2fba0e5bb3957642706bcf413e2b.png" alt=""><br>
has a high value.<br>
We can collect a dataset<br>
<img src="/blogs/joplin_resources/ec3ce0bd2e1200d228fd0620a4bff066.png" alt=""><br>
where $y_w$ and $y_l$ are winning and losing answers respectively, to tain this reward model $r_\phi(x, y)$, with the loss:<br>
<img src="/blogs/joplin_resources/138abc7bc2d706d699955ce6cad2e729.png" alt=""></p>
<p>Once we get the reward model, we use it to fine-tune our LLM $\pi_\theta(y|x)$:<br>
<img src="/blogs/joplin_resources/96472bcfc84c2a55bf6403175db2b026.png" alt=""><br>
The first term in the square bracket is to ensure that the generate answer $y$ by $\pi_\phi$ has high reward value, and the second term is a regularization to make sure that the fine-tuned model doesn’t deviate the original LLM, or the reference model, too much. Otherwise, the model will simply generate answers that are polite, friendly, etc, but without actually answering x.<br>
The above optimizaiton is not differentiable as the term under the expectation symbol is not fixed, it is actually a distribution depending on $\pi_\phi$. So we cannot use SGD to train it, since the sampling process under the expectation is not differentiable, like we saw in VAE.</p>
<blockquote>
<p>My note:<br>
In VAE, the authors use a reparameterization trick, which is feasible because they suppose the distribution under expectation is a Gaussian noise. However, the distribution of $y\sim\pi_\phi(y|x)$ is not Gaussian, so we cannot use the same strategy.</p>
</blockquote>
<p>To solve that, RLHF uses reinforcement learning. However, the authors of DPO find the analytical solution of the above optimization problem, given the reward model $r$:<br>
<img src="/blogs/joplin_resources/e2fbac7b72bec5268719b7340c8030ff.png" alt=""><br>
It is not easy to compute because of the Z.</p>
<h3 id="How-the-magic-happens">How the magic happens?</h3>
<p>So, from the above equation, we get also get an expression of the reward model r:<br>
<img src="/blogs/joplin_resources/7292267019c2ca63e669ae32ff194316.png" alt=""><br>
which is expressed by the reference model and the fine-tuned model. We can then substitute this formulation to the loss function of training the reward model, and we get<br>
<img src="/blogs/joplin_resources/c2976d913e7b311363442e611b5f68d9.png" alt=""><br>
This step actually cancels Z out, and we are <strong>directly fine-tuning our LLM model</strong>, and jump the step of training a reward model!</p>
<p>This can achieve exactly the same goal as we want to achieve in the first place, the regularization, or the KL divergence between the fine-tuned model and the reference model is implicitely included in the DPO training objectives.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Mamba</title>
    <url>/blogs/2024/07/05/Mamba/</url>
    <content><![CDATA[<h2 id="Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h2>
<span id="more"></span>
<p>Tutorial https://www.youtube.com/watch?v=8Q_tqwpTpVU<br>
Paper: https://arxiv.org/abs/2312.00752</p>
<p>The shortage of Transformer is the quatratic computational complexity about the sequence length. Also, during inference, even with KV cache, the computation cost of the later token will be significnatly larger than the early token, as the KV cache is growing. Mamba developes a linear-time sequnce modeling and the computational cost for each token during inference is just the same.</p>
<p>Training: using convolution + Inference: using recurrent network</p>
<p>The whole method is based on state space models, where the formulations are:<br>
<img src="/blogs/joplin_resources/23fde06c67274b3697e16aed1d97cf5f.png" alt=""><br>
where h(t) is the state representation at time step t (or of the t-th token), x(t) is the input at t, and y(t) is the output at t.</p>
<p>This is a differential equation, but by some discretization process like Euler method, we can convert it into a recurrent process:<br>
<img src="/blogs/joplin_resources/0ad8d2e2ca85fe420f2453af685ee947.png" alt=""><br>
so that it operates like a recurrent network, where $\mathbf{\overline{A}}$ and $\mathbf{\overline{B}}$ are calculated by<br>
<img src="/blogs/joplin_resources/d89e95508e78407f2afd3322ab1a82c6.png" alt=""><br>
where $\mathbf{\Delta}$ is the time step which are learnable paramters in Mamba.</p>
<p>If we extend h0, h1, h2, …, ht, then<br>
<img src="/blogs/joplin_resources/683335d9643b85b2bf0e4c6c27ec2203.png" alt=""><br>
It means we can actually convert it into convolutions with kernel<br>
<img src="/blogs/joplin_resources/dcc55b9116aff5f3a87c7705390d58eb.png" alt=""><br>
For example,<br>
<img src="/blogs/joplin_resources/084fbe642e5f1f7a70dcafd572246987.png" alt=""></p>
<p>The good thing is that we could train it with efficient convolution (highly parallelized) and inference it with recurrent operation (with constant computation).</p>
<p>The problem is that if we initialize A and B and regard them as parameters, and using this convolution, then A and B are shared. It also means the kernels are shared for different tokens (or different time steps), so the model cannot deal with the slective coping problem, since the model <strong>is not time-varying</strong> to selectively remember or ignore inputs depending on their content.<br>
<img src="/blogs/joplin_resources/5043a37ca9f6cf1e2c43f8345db203fc.png" alt=""></p>
<p>The solution is to use dynamic kernels that depend on the current token on-the-fly. However, it would be difficult to parallelize since the kernels are not shared. Therefore the authors propose the <strong>Selective Scan</strong> algorithm to increase parallelism and computational efficiency with GPUs.</p>
<h3 id="Performance">Performance</h3>
<p><img src="/blogs/joplin_resources/bd6832abe895c28024147c38b7e522a4.png" alt=""><br>
“Transformer++” is the strong transformer based recipe in state-of-the-art methods. Mamba performs nearly the same or superiously.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Mamba</tag>
      </tags>
  </entry>
  <entry>
    <title>CLIP 相关工作2</title>
    <url>/blogs/2024/06/26/CLIP-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C2/</url>
    <content><![CDATA[<p>Something like</p>
<ul>
<li>CLIPasso<br>
The goal is that given an image, generate a stroke image with several simple curves to represent this image.<br>
Based on some initial points, curves can be drawn to generate a simple stroke image. The function from points to the stroke image is differentiable. Then to make sure the stroke image represent the given image, two kinds of losses are designed. One loss is based on CLIP, that the CLIP image embedding from the two images should be similar. The other is geometric loss, that the features in the shallow layers of a VGG/ResNet of these two images should be similar.</li>
</ul>
<span id="more"></span>
<ul>
<li>
<p>CLIP4Clip<br>
The goal is to use CLIP to do video retrieval. Given a text, retrieve the associate videos. Very simple, there are three kinds of ways to use CLIP image encoder to get the embedding of a video. The first way, calculating the image embedding of each frame, then use avg pooling. Second way, use LSTM/1D Conv to fuse the image embedding from frames. Third way, use a cls token along with the frame embeddings and use a transformer to get the cls embedding in the output.</p>
</li>
<li>
<p>ActionCLIP<br>
Datasets are annotated with video-label pairs. The method fine-tunes CLIP, by introducing some text prompt (in order to get a complete sentence that contains the label) and image prompt (i.e., image adapters which can be trained). Similarly to CLIP4Clip, they use CLIP image encoder to get video embeddings and calculate the similarity between on video embeddings and text embeddings. The difference to CLIP is that the GT is not a diagonal matrix but also has some non-zero elements on other places. This happens when multiple videos share one action.<br>
Therefore, they can do zero-shot action recognition.</p>
</li>
<li>
<p>AudioCLIP<br>
They just use the idea of CLIP, they have video-text-audio dataset, such that three pairs are used to do contrastive learning.</p>
</li>
<li>
<p>PointCLIP<br>
They might fine-tune CLIP or just use CLIP encoders to extract text and image embeddings. Each point cloud has a label which can be converted to a sentence by some prompting. Then the point cloud are used to extract different images through different angles. Based on that, they have image-text pairs.<br>
Therefore, they can do zero-shot point cloud recognition.</p>
</li>
<li>
<p>DepthCLIP<br>
This can be quite simple, as original images can be processed by CLIP image encoder, then the GT depth image can be used to first extract sentences, by something like: this object is far/unseen/close, by setting some thresholds of the depths to define the object.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>CLIP</tag>
      </tags>
  </entry>
  <entry>
    <title>Mistral - Mistral.AI company</title>
    <url>/blogs/2024/07/03/Mistral-Mistral-AI-company/</url>
    <content><![CDATA[<p>Tutorial:https://www.youtube.com/watch?v=UiX8K-xBUpE</p>
<h2 id="Mistral-7B">Mistral 7B</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/649acb777c0f3c697b34fbf0a5210be1.png" alt=""></p>
<p>Paper: https://arxiv.org/abs/2310.06825</p>
<p><img src="/blogs/joplin_resources/276dfa772215f086120c7167554ee6b1.png" alt=""></p>
<p>For RMS, Rotary Positional Encoding, and Grouped Query Attention, we have introduced in LLaMa.</p>
<hr>
<p>Mistral is also a decoder based large language models, the goal is to predict next tokens given a prompt.</p>
<p><img src="/blogs/joplin_resources/38106b4ff7efa67b35155e6df4827a3e.png" alt=""></p>
<p>OK, the difference between Mistral and LLaMa is the sliding window attention, rolling buffer KV Cache, sparse MoE feed forward module and the SiLU function used in Mistral.</p>
<ul>
<li>Sliding Window Attention<br>
<img src="/blogs/joplin_resources/6dd38abc2796de92f6ad3929bd2c6b27.png" alt=""></li>
</ul>
<p>Instead of taking all the previous tokens when predicting the current token, sliding window restrict the the number of previous tokens the current token can see. But indirectly, the receptive field of the current token still include all the previous tokens, similar to the receptive field of features in deep layers of a CNN.</p>
<ul>
<li>
<p>Rolling Buffer KV Cache<br>
Due to the existence of sliding windows, the KV cache buffer has fixed size, and the content in the cache is updated in a rolling way.<br>
Pre-fill and chunking are techniques to implement the CV cache, in an memory efficient way. Particularly, the prompt is split into several chunks with each chunking has the size of the sliding window D. For the token to be predicted, the KV cache have values from previous chunk and current chunk, such that the memory only has two chunks of KV but ensure each token can see the previous  D tokens.</p>
</li>
<li>
<p>MoE Feed Forward<br>
<img src="/blogs/joplin_resources/22bdc3d3dd4699ad9478ef36a03b0499.png" alt=""><br>
It is like dynamic channel pruning, with a gate to decide which feed forward experts (e.g., the top 2)  are used to generate the output. To make sure the scale of the output remain stable, it uses weighted sum to add the selected experts.</p>
</li>
<li>
<p>Pipeline Parallelism<br>
Paper “GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism”: https://arxiv.org/abs/1811.06965<br>
If using model sharding, the big Mistral was split to several parts which are assigned to different GPUs in sequence. GPU1 is reponsible for Layer1-Layer4, for example. To finish the whole forward and backward pipleline, we need to wait the previous GPU to excute the next GPU. So it wastes much time.<br>
Using pipeline parallelism, we divide the mini-batch into several micro-batches, and feed micro-batches to GPUs and it can save time. For example, when we feed micro-batch 1 to GPU1, then GPU2 is free now, we can at the same time feed micro-batch 2 to GPU2, smilarly, micro-batch 3 to GPU3, etc. Then when GPU1 finished micro-batch 1, it means other micro-batches are finished by other GPUs, we can then feed micro-batch 1 to GPU2 because GPU2 is now free, and at the same time micro-batch 2 to GPU3, and so on, and we may finally feed micro-batch 4 just finished by GPU4 to GPU1 if GPU1 is free (when it doesn’t take more new batches) to make it work in a cricle.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Quantization</title>
    <url>/blogs/2024/07/03/Quantization/</url>
    <content><![CDATA[<p>Tutorial: https://www.youtube.com/watch?v=0VdNflU08yA</p>
<span id="more"></span>
<p>There wo main types of network quantizatin:</p>
<ul>
<li>Post Training Quantization<br>
The goal is to quantize the pretrained networks without fine-tuning. To quantize both activations and weights, with the goal that it maintains the accuracy.</li>
<li>Quantization aware Training<br>
To quantize the weights and activations during training, using STE to propagate gradients of quantization functions.</li>
</ul>
<p>The two common quantization function are follows:<br>
<img src="/blogs/joplin_resources/1631bc6e35b0257400bdad21799d373d.png" alt=""></p>
<p><img src="/blogs/joplin_resources/f31d6c25279038cdcab570794f59a8f1.png" alt=""></p>
<p>Let’s say Y = WX + B, then W, B and X are all quantized using certain quantization functions to quantize 32-bit floating point numbers to 8-bit integers. We can use different granularities, for examples, per-channel, per-layer, etc. But the result Y is actually integers with more bits than 8, usually it’s 32-bit. To dequantize Y back to floating-point, we need to obtain the corresponding quantization parameters alpha and beta (as shown in the figures). This can be done by sampling some values from Y and calculating them as an approximation.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Network compression</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG - Retrieval Augmented Generation</title>
    <url>/blogs/2024/06/28/RAG-Retrieval-Augmented-Generation/</url>
    <content><![CDATA[<p>Tutorial: https://www.youtube.com/watch?v=rhZgXNdhWDY<br>
Paper: https://arxiv.org/abs/2005.11401</p>
<span id="more"></span>
<h2 id="Retrieval-Augmented-Generation-for-Knowledge-Intensive-NLP-Tasks">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</h2>
<p><img src="/blogs/joplin_resources/11998ecbc4bb45d66b95964082d2d292.png" alt=""></p>
<p>Motivation: for sequence to sequence generation models like LLaMa and ChatGPT, they cannot have the latest knowledge since they were trained with old data. To answer question regarding the new knowledge, the use can input some latext documents like wikipeadia as prompt and then ask a question. These documents are usually large, making the model slow.</p>
<p>RAG is kind of method that combines the parametric memory and non-parametric memory to speed up the QA process.</p>
<ul>
<li>parametric memory<br>
knowledge stored in the s2s model, which was the original model trained with old data.</li>
<li>non-parametric memory<br>
the retriver to be discussed, that are a separate model trained to have the new knowledge.</li>
</ul>
<h3 id="Inference">Inference</h3>
<p>Given the retriever (including a query encoder and a document index/embedder), the process of QA is:</p>
<p>The query x is encoded into q(x) by the query encoder, then find the top-K document embeddings d(z) which have the highest similarities to q(x). The document embedding is the output of the document encoder that encodes the document chunks z (like sentences or document segments). Then we find the associated document chunks z according to d(z) (they have one-by-one map of course), feed both x and z to the original s2s model and obtain the output.<br>
Mathematically, we have:</p>
<p><img src="/blogs/joplin_resources/4a320578bf21c80fe6258470d4e0f068.png" alt=""></p>
<p>It reduces the generation time of the original s2s model since we only use K document segments, instead of the whole document, and x as the prompt.</p>
<h3 id="Train">Train</h3>
<p>We can train (fine-tune) both the retriever and the original s2s generator end-to-end, given data (x, y) where y is the annotations.</p>
<p>From the tutorial, we can also train the retriever separately, for example using Sentence BERT with a shared BERT architecture for query encoder and document index, to make sure that semantically similar sentence tokens have outputs that have high similarity or short Euclidean distance.</p>
<p><img src="/blogs/joplin_resources/e205d3ff6631186a6167ca1729cf471d.png" alt=""></p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Segment Anything Model (SAM)</title>
    <url>/blogs/2024/06/27/Segment-Anything-Model-SAM/</url>
    <content><![CDATA[<p>Tutorial: https://www.youtube.com/watch?v=eYhvJR4zFUM</p>
<h2 id="Segment-Anything">Segment Anything</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/efb033aa848968287780a9ccf26bc3c8.png" alt=""></p>
<p>The above figure show three most important characters of the model.</p>
<ol>
<li>The segmentation can be prompted by four types, point, bounding box, a coarse mask, and text.</li>
<li>Use a heavy image encoder to ensure the high-quality feature representation, while use a lightweight decoder to do real-time interactive segmentations depending on users’ prompts.</li>
<li>Like BLIP (but not exactly the same), after a first stage training (by using annotated data), finally the model is able to generate annotations (masks) for new images without human interference, hence creating a large scale dataset with 11 million images and 1+ billion masks and go back to train the model again. It should be noted that the initial annotations generated by the model will post-processed by techniques like NMS, threshold selection, etc. Based on that, I think it makes sense to use the generated annotations to back train the model.</li>
</ol>
<p><img src="/blogs/joplin_resources/9830305252a59939232efe53da249c1b.png" alt=""></p>
<p>The model looks like this. It supports four types of prompts, therefore it has corresponding different prompt encoders to convert the prompts to tokens that can do cross-attention with the tokens from the image. The lightweight decoder will generate three mask tokens which then give three masks. The three masks deal with the problem of ambiguity from the prompts. For example, a point can associate with different objects that have the same point included.</p>
<p><img src="/blogs/joplin_resources/1eef9054db0cc739034293a6167171f2.png" alt=""></p>
<p>This is the details of the decoder. The output tokens (in code, they have 4 output tokens)  are actually the output of the first attention blcok (there are two attention blocks), which is firtly an initialized learnable tokens, then the second attention block use the output tokens to generate 3 masks and 1 IoU score.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态学习相关工作1</title>
    <url>/blogs/2024/06/27/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C1/</url>
    <content><![CDATA[<h2 id="ViLT-Vision-and-Language-Transformer-Without-Convolution-or-Region-Supervision">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/c2ad031cca6843543f322846438f7abb.png" alt=""></p>
<p>The paper summarizes four types of vision-language learning frameworks. In computer vision, it is widely accepted that visual embedder or encoder is more important than text embedder, therefore having more computational complexity.<br>
In a), earlier works use separate embedders with light textual embedder but heavier visual embedder, and the intreaction module is just to calculate similarities between two modalities.<br>
In b), typical work is <strong>CLIP</strong>, where two embedders are both heavy but in the intreaction, they also compute the similarity to calculate contrastive loss.<br>
In c), paper like <strong>ViLBERT and UNITER</strong> use this frame.<br>
In d), this is waht <strong>ViLT</strong> proposes. Just like what ViT did, they think the image embedder can be just a simple patch tokenizer. The previous three frameworks is very time-consuming when dealing with images, but ViLT demonstrates that d) can speed up maganitudes of times and still get good results.</p>
<h2 id="Align-before-Fuse-Vision-and-Language-Representation-Learning-with-Momentum-Distillation">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</h2>
<p><img src="/blogs/joplin_resources/387d11d891035453e0efe065cfb38394.png" alt=""></p>
<ul>
<li>ITC loss (image-text contrastive loss): they propose to do contrastive learning before multimodal interaction, such that the features fed to intreaction are aligned to make interaction more effective. The loss is the same as that in CLIP.</li>
<li>ITM loss (image-text matching loss): use the output of the multimodal encoder to generate the classifying logits for a binary classification problem, to check whether the image-text pair is positive or negative. Here, they use the similarity scores from ITC to mine the hard negative pairs.</li>
<li>MLM loss (masked language modeling loss): just like BERT, the multimodal encoder will generate textual outputs/tokens and give probability in the masked place for predicting the masked word. But this time both textual and visual information are considered for MLM via the multimodal encoder.</li>
</ul>
<p>They also propose a momentum model for knowledge distillation for ITC and MLM losses. Like MoCov1, the moemtum model is updated based on the textual and visual embedders using the large momentum, then the moemntum model is used as a teacher model to generate similarities between images and texts as a second GT to guide ITC (it mean ITC considre both the one-hot GT and the outputs from teacher).</p>
<p>Similarly, the probabilities generated using teacher model for MLM is also a second GT for MLM loss.</p>
<h2 id="VLMo-Unified-Vision-Language-Pre-Training-with-Mixture-of-Modality-Experts">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</h2>
<p><img src="/blogs/joplin_resources/e3c8299f643ae637868ce3d9baca8644.png" alt=""></p>
<p>The motivation is in previous works the model cann’t do both single-modal and multi-modal tasks. For example, separately encode images and texts for retrieval tasks are singel-modal based.</p>
<p><img src="/blogs/joplin_resources/427d1b2d3a482d0b5c48d9dfe9a8071a.png" alt=""></p>
<p>They use a multi-stage traing method, to make full use of the single-modal data (unlabeled) and the image-text pairs data. As shown in the figure, in each stage, some parameters are frozen.</p>
<p>So one advanntage of VLMo is that they use only one transformer architecture but separate light experts.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Multimodal learning</tag>
      </tags>
  </entry>
  <entry>
    <title>视频理解串讲-1</title>
    <url>/blogs/2024/08/01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%B2%E8%AE%B2-1/</url>
    <content><![CDATA[<p>Video: https://www.youtube.com/watch?v=gK7AGO6okhc</p>
<p>视频理解在深度学习时代有过已下几大探索：<br>
<img src="/blogs/joplin_resources/af77d0b37bef5a3bea77972618e9df7d.png" alt=""></p>
<span id="more"></span>
<h3 id="1-早期CNN-DeepVideo">1. 早期CNN - DeepVideo</h3>
<p>对于早期CNN方法，代表是DeepVideo, 探索了很多我们早期可能会第一时间想到的各种可能，比如用2D CNN对每帧图像进行处理，结合early fusion, late fusion，slow fusion等等。<br>
<img src="/blogs/joplin_resources/32a7cb5500314e537c955f080ce8f8f0.png" alt=""><br>
答案就是效果不好。</p>
<h3 id="2-双流网络">2. 双流网络</h3>
<p>加上时序信息，能显著提升视频理解性能（主要是在动作识别这一任务上进行的探索）<br>
<img src="/blogs/joplin_resources/73da16bf45d6e404b9326d0a69582bc3.png" alt=""></p>
<p>The method is quite simple, first use some basic method to extract the optical flow for video which is then used as temporal information.<br>
The first stream use 2D CNN to process single image frames, the input of which is a single image, and the network is a pretrained CNN on ImageNet.<br>
The second stream use a sequence of optical flow frames (with a fixed number, e.g., 20), and train the network from scratch (because the number of input channels are now becoming 20 instead of 3).<br>
Finally, the output of softmax from the two streams are averaged to give the final score.</p>
<p>The performance is increased from like 60% to 80%, comparable with the performance of the best traditional handcrafted feature based methods. Typical datasets are UCF-101, and Sports-1M.</p>
<h4 id="How-to-test">How to test?</h4>
<p>Well, they sample a fixed number of frames from each test video, no matter how long the video is. Then they use corner crops and center crop for each video frame, plus flipping, getting 10 times bigger than the original sampled test data. For each 1 of the 10 versions of video, they use this two-stream network to get a score, and do averaging to get the final score.</p>
<h4 id="Tips">Tips</h4>
<ol>
<li>They use mean subtraction to alleviate the camera motion.</li>
<li>They use bidirectional optical flows (forward and backward, respectively).</li>
</ol>
<h4 id="Other-possible-tricks-also-appear-in-different-papers">Other possible tricks (also appear in different papers)</h4>
<ol>
<li>Combine space fusion and temporal fusion. Space fusion is implemented by simpling summation, concatenation, or stack-and-conv operations to fuse features from the two streams. For temporal fusion, we could use 3D conv, or pooling to fuse features from consecutive frames.</li>
<li>Initialization of temporal network from pretrained 2D network on ImageNet. Since the number of input channels of the network is changed from 3 to 20, an effective way to initialize the network for the first layer is to firstly averaging the 3 channels’ corresponding parameters and repeat it 20 times.</li>
<li>Partial BN, which only learn the BN parameters for the first BN, and keep the rest of the BNs frozen.</li>
</ol>
<h4 id="How-to-effectively-deal-with-long-videos">How to effectively deal with long videos?</h4>
<h5 id="Future-work-1-Beyond-short-snippets">Future work 1: Beyond short snippets</h5>
<p>It is a paper to solve the problem:<br>
The initial idea would be using pooling to aggregate the information from multiple frames. So they have investigated like max pooling, average pooling, and Convolutional pooling.<br>
Finally, they use <strong>LSTM</strong> to do the pooling. But the effect was not that good, it might because that the test videos like in UCF-101 are too short, repeatedly feedinding features of video frames that might be very similar with each other, can not fully utilize the advantage of LSTM.</p>
<h5 id="Future-work-2-Temporal-Segment-Networks-TSN-Towards-Good-Practices-for-Deep-Action-Recognition">Future work 2: Temporal Segment Networks (TSN):Towards Good Practices for Deep Action Recognition.</h5>
<p>This paper is quite straightforward but absolutely effective. 简单有效！ECCV 16.</p>
<p><img src="/blogs/joplin_resources/a48581d5d261c8f6efa32aec7d6cec71.png" alt=""></p>
<p>So they segment the long video into pieces. For each segment, they sample a frame, and a sequence of optical flow frames based on that frame, generating the space logits and temporal logits through a two-stream network. Then they aggregate all the space logits together, so called segmental consensus, by pooling, summation, or even LSTM to get the space-related score. Similarly, they aggregate the temporal logits to get a temporal-related score. Then they fuse the scores to get the final prediction.</p>
<p>My comments (may not from the paper): For long video, it is possible to sparsely segment the video, or increase the length of each segment.</p>
<p>The above is <strong>supervised learning</strong>. It can also do <strong>unsupervised contrastive learning</strong>, the work done by team of 朱毅 (the tutorial speaker)<br>
Generally, they have three segments, and they extract a image frame and a sequence of optical flows from each segment, obtaining 3 image frames and the corresponding 3 optical flow sequences, being one sample of a pair. Then they repeat the process to obtain another 3 images and 3 optical flow sequences, being the other sample of the pair. So this pair can be regarded as a positive pair, because they are extracted from the same segment group. Similarly, they can extract a negative pair by using two different segment groups.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Video understanding</tag>
      </tags>
  </entry>
  <entry>
    <title>视频理解串讲-2</title>
    <url>/blogs/2024/08/01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%B2%E8%AE%B2-2/</url>
    <content><![CDATA[<p>Video: https://www.youtube.com/watch?v=J2YC0-k57NM</p>
<h3 id="C3D">C3D</h3>
<p>3D convolution, easy to understand. Circumvent the use of optical flow, which is time-consuming. But 3D conv itself is also time-consuming with heavy computations.</p>
<span id="more"></span>
<h3 id="I3D">I3D</h3>
<p>Inflated 3D Conv, the advantage is that it can be initialized from pretrained 2D CNNs, where the 2D kernels are inflated into 3D kernels, by copying the parameters T times and dividing it by T (T is the termporal dimension of the kernel).</p>
<h3 id="Non-local">Non-local</h3>
<p>It is similar to self-attention, can be formed in spatial space or temporalsptial space.</p>
<h3 id="R-2-1-D">R(2+1)D</h3>
<p>Factorize 3D conv into 2D in space and 1D in time.</p>
<h3 id="SlowFast">SlowFast</h3>
<p><img src="/blogs/joplin_resources/9f450a9be0cbf2ae169c342003e79a82.png" alt=""></p>
<h3 id="TimesFormer">TimesFormer</h3>
<p>Decompose the 3D attentions into 2D attentions in space dimensions and 1D attentions in time dimension.</p>
<h3 id="Thoughts-about-how-to-do-long-video-understanding">Thoughts about how to do long video understanding</h3>
<ol>
<li>Transformer, Non-local to capture long-term dependencies.</li>
<li>RNN like structures, like LSTM, Mamba, and TTT.</li>
<li>Local-global attentions (MobileViT, Swin Transformer, etc.) to save computation.</li>
<li>Use factorized/decomposed operators (like R(2+1)D, TimesFormer, etc.) to save computation.</li>
<li>For the case the number of tokens is huge, then using techniques like sampling, or sliding window to do self-attention.</li>
</ol>
<p>Local-global attention is that we do self-attention in local tokens, and then select local representative tokens for global self-attention.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Video understanding</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA - Google released on Feb 2023</title>
    <url>/blogs/2024/06/28/LLaMA-Google-released-on-Feb-2023/</url>
    <content><![CDATA[<p>tutorial: https://www.youtube.com/watch?v=Mn_9W1nCFLo&amp;t=27s</p>
<h2 id="LLaMA-Open-and-Efficient-Foundation-Language-Models">LLaMA: Open and Efficient Foundation Language Models</h2>
<span id="more"></span>
<p align="center">
  <img src="/blogs/joplin_resources/08eb4cd4872e73c9be305bcf153afdc0.png" width="60%">
</p>
<p>This is the framework drawn by Umar from his video tutorial. There are several concepts from LLaMa.</p>
<ul>
<li>
<p>Rotary positional embedding [1]<br>
It was proposed by Jianlin Su et al. In the origninal method that uses absolute positional embeddings, the positional embeddings are added to the feature embeddings. In the methods using relative positional embeddings, the positional embeddings are also added to the dot product of Q and K. However, they think that they can integrate the relative distance of two tokens into the dot product, or generally the inner product of Q and K. Baased on that, they define a new inner product on the complex space where the relative distance or positions can be represented by a rotation operation.</p>
</li>
<li>
<p>RMS Norm (Root Mean Square Normalization) [2]<br>
It was suggested that the stability of the training is mostly attributed to the re-scaling of variance rather than the re-centering using means. Therefore, they only focus on variance and save the computation. The network will still achieve the same effect as Layer Norm.<br>
<img src="/blogs/joplin_resources/244ed736a78e6b3b6d909e1f2a718b9f.png" alt=""></p>
</li>
<li>
<p>KV Cache<br>
<img src="/blogs/joplin_resources/97b7324cb7b6321d2f61645e459fd42e.png" alt=""></p>
<p>In the task of next token prediction, for each iteration, we only care about the last output token since the previous tokens have already been predicted. But we calculate these tokens in every iteration repeatedly. To avoid that, for each iteration and suppose we have to predict token 3, then we append the token 3 from K to the K buffer, since token 1 and 2 are already there, and append the Token 3 from V to the V buffer, and after the attention layer calculation, we obtain the prediction of token 3.</p>
</li>
<li>
<p>Multi-Query Attention  [3] and Grouped Multi-Query Attention [4]<br>
As we have reduced the computation by KV cache, the bottleneck which affects the speed is now becoming the Memory access, according to the ratio of memory and arithmetic operations. So they only divide the Q into multi heads and use a single shared head for the case of K and V. But it will degrads the model slightly, so they use grouped meahcnism like grouped convolution. They are illustrated below.<br>
<img src="/blogs/joplin_resources/00ba5684d55220456d1b2630d45b9a34.png" alt=""></p>
</li>
</ul>
</br>
</br>
</br>
</br>
<p>[1] <a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></p>
<p>[2] <a href="https://arxiv.org/abs/1910.07467">Root Mean Square Normalization</a></p>
<p>[3] <a href="https://arxiv.org/abs/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a></p>
<p>[4] <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态学习相关工作2</title>
    <url>/blogs/2024/06/27/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C2/</url>
    <content><![CDATA[<h2 id="BLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h2>
<span id="more"></span>
<p>The paper has two main controbituions, one is to use a decoder to caption the image, this is what previous works didn’t do since they only use transformer encoders. The other is use the trained BLIP to augment datasets. Particularly, they generate synthesized captions, then use caption filters to filter out those wrong image-text pairs. Finally, they obtain a larger scale dataset and get back to train the model again.</p>
<p><img src="/blogs/joplin_resources/c9be2378731b79df21daa89c3bab7aae.png" alt=""></p>
<p>During training, they use three losses like ALBEF, but replace the MLM loss to LM (language modeling) loss to predict next word like in original Transformer. In addition, the parameters (the same color in the figure indicates the shared structure parameters) are shared when training with different losses. The causal self-att is actually using decoders.</p>
<p><img src="/blogs/joplin_resources/0985c805775281b7dc2986410146d8e8.png" alt=""></p>
<p>To augment data, here, {Ih, Th} are the created image-text pairs, and those green colors mean the corrected or filtered captions.</p>
<p>BLIP can be used in my tasks including single modal-based ones and multi-modal-based ones like question answering, because they have single-modal encoders and modal interaction embedders, and also decoders.</p>
<h2 id="CoCa-Contrastive-Captioners-are-Image-Text-Foundation-Models">CoCa: Contrastive Captioners are Image-Text Foundation Models</h2>
<p>The drawbacks of BLIP and many other methods like VLMo, ALBEF are that they need to inference the language encoders as many times as the number of losses, because each losses may use different language inputs. For example, MLM need the words to be masked.</p>
<p>Hence, CoCa proposes to only inference the encoder once during training.</p>
<p align="center">
  <img src="/blogs/joplin_resources/5543e2e85ba71d18b5d99b6207a472b2.png" width="40%">
&nbsp; &nbsp; &nbsp; &nbsp;
  <img src="/blogs/joplin_resources/f6499769e1bfab743b52ac687e2afb3b.png" width="55%">
</p>
<p>It has the ICT loss and ML loss, but the textual embedder is a transformer decoder where casual mask attention is used. The representation of text is from the last cls-token of the decoder which can see all the words in the text. The performance of CoCa is amazing!</p>
<h2 id="BEiTv3-Image-as-a-Foreign-Language-BEiT-Pretraining-for-All-Vision-and-Vision-Language-Tasks">BEiTv3: Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</h2>
<p><img src="/blogs/joplin_resources/f5b3c561af8b17a99990d6c7926cb0a2.png" alt=""></p>
<p>The framework is even simpler. They treat image as language (Imaglish) and the shared self-attention blocks are trained only using masked data modeling loss. For language, the loss is MLM like in BERT/ALBEF/VLMo, for image, the loss is like in MAE, for multi-modal data, I guess is also to predict the language word/image patch based on multimodal input.</p>
<p><img src="/blogs/joplin_resources/4b0686d5c19b686c6430c7c4b5e1ad05.png" alt=""></p>
<p>They can tasks like listed above.  The performance beats that of CoCa.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Multimodal learning</tag>
      </tags>
  </entry>
  <entry>
    <title>How to check remote hexo webpages in our local computer</title>
    <url>/blogs/2024/12/28/How-to-check-the-blog-webpages-in-our-local-comput/</url>
    <content><![CDATA[<h3 id="Basics-of-Hexo">Basics of Hexo</h3>
<h4 id="Basic-knowledge-before-developing-hexo-project">Basic knowledge before developing hexo project</h4>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Full name</th>
<th>Purpose in Hexo</th>
<th>Key Benefits</th>
</tr>
</thead>
<tbody>
<tr>
<td>NVM</td>
<td>node version manager</td>
<td>Manage Node.js versions for Hexo projects.</td>
<td>Avoid compatibility issues, work with multiple projects requiring different Node.js versions.</td>
</tr>
<tr>
<td>NPM</td>
<td>node package manager</td>
<td>Install Hexo, plugins, and dependencies.</td>
<td>Streamlined dependency management, consistent environment across systems.</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<p>Basically, use nvm to install the latest nodejs. You can type “nvm current” to show the current used version of nodejs in your computer. The version of nodejs sometimes is important, for exmpale, some higher versions of nodejs may cause many strange problems when developing hexo project, down-gradeing it to a lower version may solve the problems. Currently, the computer (203.205 computer in university of Oulu) uses version v20.10.0. It works fine.</p>
<p>Then just use the latest version of npm, which can be done by “npm install -g npm@latest”. The “-g” option install the package globally. Besides, you also need “npm install -g hexo-cli” to install hexo-cli package, which is essential for developing hexo projects. You could also modify “hexo-cli” to “hexo-cli@latest” to install the latest version of hexo-cli package.</p>
<p>After hexo-cli is installed, you need to init a hexo project (my project’s name is “zhuo-blog”, so let’s use it) by “hexo init zhuo-blog” and it will crate a directory “zhuo-blog” which is your hexo project root dir.</p>
<p>Now, enter the dir “cd zhuo-blog”. You could see a file named “package.json” that lists all the needed packages with specific versions for the project. Not sure how the versions of those packages are determined, maybe by the specific versions of nodejs, npm, hexo-cli, etc.</p>
<p>Therefore, you need to install all the needed packages by simply running “npm install”. Then npm will automatically install all the packages (by reading package.json), not globally, but only in your project directory. The installed packages will be stored in the created “node_modules” folder.</p>
<p>You can install more packages to enrich your project’s functionalities by “npm install package-name”.</p>
<p>Modify the _config.yaml file to configure your expected functions.</p>
<h4 id="Choose-a-theme">Choose a theme</h4>
<p>I think there is a default theme for the project. However, you could always change the theme by setting it in _config.yaml under the field “theme: xxx”, provided that you have already git clone the theme repsitory under “themes” folder (alternatively, you can also install the theme by following the installation instructions in the official website like github readme page).</p>
<p>I am using “next” theme, I guess the version is 8.21.1. I just use git clone to clone the repository to “next” folder under “themes” folder for installation.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> themes</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/next-theme/hexo-theme-next next</span><br><span class="line"><span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure>
<p>next also has its own _config.yaml, where you could add particular rendering functions for your needs.</p>
<h4 id="Finally-commands-to-develop-hexo-blogs">Finally, commands to develop hexo blogs</h4>
<blockquote>
<p>hexo new post “blog name”</p>
</blockquote>
<p>This command create a blank post “blog-name.md” under “sources/_posts” folder for you to edit. It uses markdown edditing format.</p>
<blockquote>
<p>hexo generate</p>
</blockquote>
<p>After edited your md file, this command will update “public” folder (if not exists, a new one will be generated) with all the static files where you could open in your brower with “localhost:port”, with the following command.</p>
<blockquote>
<p>hexo server</p>
</blockquote>
<p>It create a link for you to browse in the local brower, where you will see the blog webpages. If you are not satisfied with the webpage, modify your md file, and run “hexo generate” again, and refresh your webpage. You don’t need to run “hexo server” again.  Alternatively, you can run “hexo generate --watch” such that it keeps generating whenever it detects md file is modified.  You can also run “hexo clean” to completely remove the “public” folder follwed by “hexo generate”. But if the modification of your md file is minor, it is not necessary.</p>
<blockquote>
<p>hexo deploy</p>
</blockquote>
<p>When you are satisfied, run this command to deploy your blogs to github (the github address is defined in _config.yaml of the project).</p>
<h3 id="Environment-setting-1-1-pc">Environment setting 1 (1 pc)</h3>
<p>Suppose we are developing hexo blogs in a pc and we wan to check the website effect of our blogs (in this case, only one pc is involved).</p>
<p>It is quite simple. Just like above, run “hexo server” and click the link to check the webpage in this computer.</p>
<h3 id="Environment-setting-2-2-pcs">Environment setting 2 ) (2 pcs)</h3>
<p>Supposing you are using your laptop, but the hexo project is located in a remote server, you want to open the webpage in your laptop browser. In this case, you need port forwarding. Then in the remote server, you have to run:</p>
<blockquote>
<p>hexo server --host 0.0.0.0 --port 4000</p>
</blockquote>
<p>0.0.0.0 allows the project to be opened in a remote computer (which is your laptop, because the remote server becomes “local” for the project). The port number can be any as long as it is not used anywhere.</p>
<p>Now, in your local laptop, you have to run</p>
<blockquote>
<p>ssh -f -N -L 4001:localhost:4000 username@remote-server-ip</p>
</blockquote>
<p>This command enables any traffic sent to port 4001 on your local machine is forwarded to port 4000 on the remote server via the SSH tunnel.<br>
After that, open http://localhost:4001 in your browser to see the blogs.</p>
<p>Here, -f means run in the background, -N means no command is executed in the remote server.</p>
<h3 id="Environment-setting-3-3-pcs">Environment setting 3 (3 pcs)</h3>
<p>Now, 3 pcs are involved: pc1, pc2, pc3. pc1 is your laptop, pc2 is the middle remote server, pc3 is the remote server that has the hexo project. Similarlly:<br>
In pc3:</p>
<blockquote>
<p>hexo server --host 0.0.0.0 --port 4000</p>
</blockquote>
<p>open another terminal in pc3 and run</p>
<blockquote>
<p>autossh -fNR 4001:localhost:4000 username@middle-server-ip</p>
</blockquote>
<p>Then traffic sent to 4001 in middle server (pc2) will be forwarded to 4000 in pc3.</p>
<p>In your laptop, run</p>
<blockquote>
<p>ssh -f -N -L 4002:localhost:4001 username@middle-server-ip</p>
</blockquote>
<p>Doing so, traffic sent to 4002 on your laptop will be forwarded to 4001 in middle server, and then forwarded to 4000 in pc3. You can open http://localhost:4002 in your laptop browser to see the blogs.</p>
<ul>
<li>Note: in the above process, you don’t run commands in pc2.</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ZeRO (DeepSpeed from Microsoft)</title>
    <url>/blogs/2025/01/01/ZeRO-DeepSpeed-from-Microsoft/</url>
    <content><![CDATA[<h2 id="ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</h2>
<p>paper (2019 arxiv, SC’20): <a href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a><br>
website: <a href="https://www.deepspeed.ai/">https://www.deepspeed.ai/</a></p>
<span id="more"></span>
<h3 id="Overivew-of-ZeRO-optimizers">Overivew of ZeRO optimizers</h3>
<p><img src="/blogs/joplin_resources/d7080c98a06ed87132146d152825625a.png" alt=""></p>
<p>So basically, ZeRO has two sets of optimizations: <code>ZeRO-DP</code> to reduce the memory footprint along with data parallelism (like optimizer states by $P_{os}$, gradients by $P_g$, and parameters by $P_p$), and <code>ZeRO-R</code> to reduce the residual memory (like activations by $P_a$, temporary buffers by $C_B$, and memory fragementation by $M_D$). Particularly, activation are reduced along with model parallelism (I would say might be implemented as “sequence parallelism” in Megagron).</p>
<p>The highlight is that ZeRO-DP can significantly reduce the memory per GPU while keeping the communication overhead between GPUs unchanged when only applying DP alone (without the need of MP), therefore allow bigger models to train efficiently.<br>
<img src="/blogs/joplin_resources/5a8f4be1364e8b6e41e42537eb06c768.png" alt=""></p>
<p>For example, as shown in above figure, Megagron-LM (which only applying tensor model parallelism here) will need to use multiple nodes because a single node (16 GPUs) cannot fit a bigger model (larger than 40B), causing inter-node communications that are very slow. However, for ZeRO, MP always fits in a node. Megatron make the computation granularity smaller (by splitting the model into smaller pieces) while ZeRO keeps every whole layer of transformer (my opinion). In addition, ZeRO also performs better than Megatron becuase the memory on each DP GPU is reduced without increasing communication overhead (as can be seen in the 1.5B case).</p>
<p>By adjusing the degree of ZeRO-DP, degree of MP, batch size, etc, ZeRO can gives good scalability for large models, even super-linear scalability:<br>
<img src="/blogs/joplin_resources/8557e921935a44025263389ac9a74fc6.png" alt=""><br>
It is because $P{os+g}$ reduces per GPU memory consumption of ZeRO-100B with increase in DP degree, allowing ZeRO-100B to fit larger batch sizes per GPU, which in turn improves throughput as a result of increasing arithmetic intensity. For example, we assume MP is fit in a node with 16 GPUs, thus the degree of MP is bounded to 16, then the degree of DP can be increased with more GPUs.</p>
<p>So let’s describe more about the methods.</p>
<h3 id="Let’s-see-which-occupies-the-GPU-memory-during-training">Let’s see which occupies the GPU memory during training</h3>
<p>Let’s assume the training is via mixed precision (fp16/32) training where the parameters and gradients are stored in 16 bit (2bytes) while the optimizer states use 32-bit (4bytes). Let the number of parameters is $\Psi$ and Adam optimizer is used, then here is the memory consumption:</p>
<ul>
<li>Parameters: $2\Psi$</li>
<li>Gradients: $2\Psi$</li>
<li>Optimizer states: parameters + momentum + variance cuase $4\Psi + 4\Psi + 4\Psi = 12\Psi$</li>
<li>Activations: It depends on the sequence length, batch size, and the structure of transformer models (please also see <a href="/blogs/2024/12/26/Megatron-LM-3">Megatron-LM series 3</a>). For example, GPT-2 with 1.5B parameters, 1K sequence length, and batch size 32 causes 60G memory.</li>
<li>Temporary buffers: Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput.</li>
<li>Memory fragmentation: It is caused by the interleaving between short lived and long lived memory objects. For example, the activations checkpointed (uisng activation checkpointing) lives longer than those need to be recomputed during forward pass. The gradients of parameters live longer than the gradients of activations during backward pass. A long contiguous memory will be fragmented into small pieces when the interleaved short memory has been released. So it will still possible to cause OOM error when the total available memory is larger than requested because of the lack of longer enough contiguous memory.</li>
</ul>
<h3 id="ZeRO-DP-to-reduce-the-first-three-memory-parts-parameters-gradients-and-optimizer-states">ZeRO-DP to reduce the first three memory parts (parameters, gradients, and optimizer states)</h3>
<p><img src="/blogs/joplin_resources/8fdb71dae74d76259cc162100ddd3d80.png" alt=""><br>
As shown in the above figure, ZeRO has three stages $P_{os}$, $P_{os+g}$, and $P{os+g+p}$ that gradually reduce the memory consumed per GPU ($N_d$ is the degree of data parallelism).</p>
<h4 id="ZeRO-DP-P-os">ZeRO-DP $P_{os}$</h4>
<p>For $P_{os}$, the idea is quite simple. In this case, only the optimizer states are partioned to different DP GPUs, but all the parameters are replicated in each GPU. During forward and backward passes, the each DP GPU run for its own minibatch and get the loss via forward pass and gradient via backward pass. Then the gradients are kind of processed with “reduce-scatter” to the optimizers in each GPU (first reduce the gradients to calculate the average, and scatter the gradients to different pieces and give each GPU a piece). After that, each GPU run optimizer.step() to update the corresponding parameters. Finally, the “all-gather” operation is used to gather different pieces of updated parameters as the new parameters on each GPU.</p>
<blockquote>
<p>Communication overhead<br>
Compared with the original way of DP, the “reduce-scatter” and “all-gather” operations cause the same communication overhead as the “all-reduce” operation in the original DP.</p>
</blockquote>
<h4 id="ZeRO-DP-P-os-g">ZeRO-DP $P_{os+g}$</h4>
<p>For gradients, the forward pass is the same as above, but for the backward pass, whenever the gradients of a certain part of the parameters are calculated, use “reduction” operation for the gradients on different GPUs to get the average and store the average on the associated GPU only. After backward, each GPU has its own gradients part for a certain par of parameters, means the GPU can directly apply optimizer.step() to update those parameters. After tha, “all-gather” is used to update all parameters on each GPU.</p>
<blockquote>
<p>Communication overhead<br>
Put all the “reduction” operations together gives the same amount of communication as the “reduce-scatter” operation based on all gradient in $P_{os}$, plus “all-gather”, it give the same communication as the original DP.</p>
</blockquote>
<h4 id="ZeRO-DP-P-os-g-p">ZeRO-DP $P_{os+g+p}$</h4>
<p>Each DP GPU only store a certain part of parameters further. When the parameters outside of its partition are required for forward and backward propagation, they are received from the appropriate data parallel process through broadcast. The rest is the same as $P_{os+g}$.</p>
<blockquote>
<p>Coummnication overhead<br>
As stated by the authors, these extra broadcast operations cuases $1.5\times$ overhead compared with the previous two cases.</p>
</blockquote>
<p>When $P_{os+g+p}$ is applied, the memory per GPU can be reduced by $N_d\times$, enabling very large models (1 trillion parameters) to fit in the GPU cluster with 1024 GPUs:</p>
<p><img src="/blogs/joplin_resources/dd7fc0c7103eb20c0510374bd42e44a6.png" alt=""><br>
Here, with 1024 DP, each GPU only consumes 15.6 G memory for a 1 T model, which is lower than its limit (i.e., 32G for V100).</p>
<h3 id="ZeRO-R-to-reduce-residual-memry">ZeRO-R to reduce residual memry</h3>
<h4 id="ZeRO-R-P-a-and-P-a-cpu">ZeRO-R $P_a$ and $P_{a+cpu}$</h4>
<p>In my opinion, $P_a$ would be similar to <code>sequence parallelism</code> in Megatron-LM, but ZeRO might partition the activations along different dimensions along MP. Then $P_{a+cpu}$ additionally store the activations in CPU to free GPU memory.</p>
<h4 id="ZeRO-R-C-B-and-M-D">ZeRO-R $C_B$ and $M_D$</h4>
<p>For $C_B$, they  simply use a performance-efficient constant-size fused buffer when the model becomes too large. For $M_D$, ZeRO does memory defragmentation on-the-fly by pre-allocating contiguous memory chunks for activation checkpoints and gradients, and copying them over to the pre-allocated memory as they are produced. MD not only enables ZeRO to train larger models with larger batch sizes, but also improves efficiency when training with limited memory.</p>
<h3 id="Conlusion">Conlusion</h3>
<p>ZeRO goes with DP, so from the perspective of model parameters, it doesn’t refactor the archiecture like MP. It can be combined with MP like Megatron to further scale the training while outperforming combining DP and MP alone.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSpeed-MoE</title>
    <url>/blogs/2025/01/15/DeepSpeed-MoE/</url>
    <content><![CDATA[<h1>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</h1>
<p>paper （2022 arxiv): https://arxiv.org/abs/2201.05596</p>
<h2 id="First-let’s-look-at-how-MoE-architectures-look-like">First, let’s look at how MoE architectures look like?</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/56510c03ab3f663e61ef5ad6d7b26b27.png" alt=""></p>
<p>As shown above, basically, each MoE has a corresponding dense base model. For example, let the 350M dense model be the base model, then on every other feedforward layer, the feedforward layer is expanded with multiple branches (e.g., 128) where each feedforward branch can be seen as an expert on this layer, leading to a MoE architecture (e.g., 350M + MoE-128).</p>
<p>The number of parameters of MoE is tens times of bigger than the corresponding base model, but the training or inference cost is similar to the base model (assuming that the number of training tokens keeps the same), because only a single expert on each expert layer is activated for each token, via a gating function.</p>
<h3 id="What-other-methods-do-we-think-about">What other methods do we think about?</h3>
<ul>
<li>Network pruning, especially channel pruning (we only activate a certain part of channels in each layer).</li>
<li>Dynamic pruning, dynamic routing, where a gating function is used to make decisions about which channels, or which layers, will be activated.</li>
</ul>
<h3 id="What-are-we-expecting">What are we expecting?</h3>
<ol>
<li>We expect that the quality of a MoE model should match the quality of a dense model that is much bigger than the MoE’s base model.</li>
<li>Yeah, now maybe just point 1.</li>
</ol>
<h2 id="Second-lets’-train-all-the-dense-or-MoE-models">Second, lets’ train all the dense or MoE models.</h2>
<h3 id="First-how-we-train-the-big-MoE-model">First, how we train the big MoE model?</h3>
<p>Simple, use data parallelism and expert parallelism. For data parallelism, ZeRO series are a good option. For expert parallelism, for each feedforward layer with experts, just partition the parameters along the expert dimension, for example, each GPU has an equal number of experts.</p>
<p>Since data parallel and expert parallel training are used, before feed the data to experts, we should use a <code>all-to-all</code> operator where we collect the tokens that should be</p>
<h3 id="Do-we-get-our-expectations">Do we get our expectations?</h3>
<h4 id="Regarding-prediction-performance">Regarding prediction performance</h4>
<p><img src="/blogs/joplin_resources/6512ce37e9868e657baf926fcec2c506.png" alt=""></p>
<p>As shown above, the 350M+MoE-128 performs on par with the 1.3B dense model. Similarly, the 1.3B+MoE-128 performs on par with the 6.7B dense model.</p>
<h4 id="Regarding-training-cost">Regarding training cost</h4>
<p><img src="/blogs/joplin_resources/917f9e143d592408f47757ff42630b5e.png" alt=""></p>
<p>1.3B+MoE-128 model achieves 5$\times$ efficiency compared with the 6.7B dense model.</p>
<h3 id="What-can-we-imagine">What can we imagine?</h3>
<p>We can train GPT-3 or MT-NLG 530B quality model with a 5$\times$ reduction in training cost, via MoE models, assuming that the scaling holds.</p>
<h2 id="Third-PR-MoE-and-MoS">Third, PR-MoE and MoS</h2>
<h3 id="PR-MoE-Pyramid-Residual-MoE-for-smaller-size-and-fast-inference-but-same-quality">PR-MoE: Pyramid-Residual-MoE for smaller size and fast inference but same quality</h3>
<p><img src="/blogs/joplin_resources/f986b67fad17bfaa3a8340da0a4bb94a.png" alt=""><br>
Here is the structure for PR-MoE. The first idea is that the deeper layers could be assigned with more experts while the shallower layers less, similar to the basic CNN structures.<br>
The second idea is that we can fix one expert and dynamically select the second expert (as the residual) in the expert layers, in order to get the same quality as a Top-2 MoE.</p>
<h4 id="But-how-to-train-PR-MoE-in-parallel">But how to train PR-MoE in parallel?</h4>
<p>Solution: <strong>flexible</strong> multi-expert and multi-data parallelism design that allows for training different parts of the model with different expert and data parallelism degree. For example, we have three parts in the architecture with 32, 64, and 128 experts respectively, and the number of GPUs available is 128. The design of parallelism degree could be:</p>
<table>
<thead>
<tr>
<th></th>
<th>Non-expert parameters</th>
<th>Expert parameters in part with 32 experts</th>
<th>Expert parameters in part with 64 experts</th>
<th>Expert parameters in part with 128 experts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Expert parallelism degree</td>
<td>N/A</td>
<td>32</td>
<td>64</td>
<td>128</td>
</tr>
<tr>
<td>Data parallelism degree (I guess using ZeRO)</td>
<td>128</td>
<td>4</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Note: The training process should be like this: for example, for the layers with 64 experts, each GPU has 1/128 part of the global batch as its minibatch for its non-expert parameters. For non-expert parameters, lets’ say the index of the expert in this GPU is i, the all-to-all operator will assign all tokens belonging to expert i based on the global batch, and the tokens will be partitioned into half and one half will be put to this GPU (because now the data parallelism degree is 2). It means the data fed to non-expert and expert parameters could be from different samples from different minibatches. During the backward propagation, the gradients are averaged following the index information of which data is fed to which parameter. Please also refer to Figure 7 below.</p>
</blockquote>
<h3 id="MoS-Mixture-of-Students-the-student-is-now-a-MoE-model-too">MoS: Mixture-of-Students: the student is now a MoE model too</h3>
<p>The authors state that with standard KD where the student is a dense model, the distilled student model loses the sparse fine-tuning and inference benefits provided by MoE. So they make the student also a MoE model. To make the student smaller, they reduce the depth of each expert branch in the teacher model. The general KD loss is used that forces the MoS to imitate the outputs from the teacher MoE on the training dataset.</p>
<h4 id="A-strategy-on-training-MoS">A strategy on training MoS</h4>
<p>MoS is trained in two stages, where the first stage is the normal training with standard loss and KD loss. The second stage stops KD and only train with the standard loss. The motivation is that the student cannot perform as good as the teacher at the end of the training, and the authors hypothesize that the student don’t have enough capacity to minimize both losses, so it should focus on only one loss at the end.</p>
<h2 id="Finally-DeepSpeed-MoE-inference">Finally, DeepSpeed-MoE inference</h2>
<p>It should be in the same spirit as in training PR-MoE. Here is the figure illustration:<br>
<img src="/blogs/joplin_resources/b0d5a6393bdb235e2ac0b1edaca8e538.png" alt=""></p>
<p>What I guess the process is:<br>
in non-expert parameters, data parallelism and tensor-slicing model parallelism are used, the all-reduce is for model parallelism to average the output from different tensor slice before going to the next layer (just like Megatron-LM).<br>
The next layer might be the router to calculate the probabilities for different experts for the following expert parameters. Then because we have used 4-degree data parallelism, but the parallelism for expert parameters doesn’t use it, so we need to use “allgather” to gather the outputs from different data parallelism groups, so that each expert parallelism group see all the data, and takes the corresponding tokens that it is allocated according to the router response for each token. In each expert parallelism group, it further applies expert-slicing to increase parallelism by vertical/horizontal partitioning of expert parameters, and use “all-reduce” to average the output. Among different expert parallelism groups, it uses “alltoall” to update the information.</p>
<blockquote>
<p>The authors also designed some subsystems to optimize communications for the collective operators.</p>
</blockquote>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-Turing NLG 530B</title>
    <url>/blogs/2025/01/18/Megatron-Turing-NLG-530B/</url>
    <content><![CDATA[<h1>Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</h1>
<p>paper (2021 arxiv): <a href="https://arxiv.org/abs/2201.11990">https://arxiv.org/abs/2201.11990</a></p>
<h2 id="Model-architecture-and-training-hyperparameters">Model architecture and training hyperparameters</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/5ea3e82602184dbccc6d6e7c72d1559d.png" alt=""></p>
<p>The model has 510B parameters scaled from the transformer decoder. Sequence length is 2048, and the global batch size is 1920.</p>
<ul>
<li>LR: use 1B tokens to linearly warmup LR to $5.0e^{-5}$. The LR is then decayed to 10% of its value over 340B tokens with cosine decay.</li>
<li>Batch size: starts at 32 and gradually increases to 1920 with a step of 32, in the first 12B tokens.</li>
<li>Weight initialization: normal distribution with 0 mean and standard deviation of $4.0e^{-3}$</li>
<li>Optimizer: Adam, $\beta_1=0.9$, $\beta_2=0.95$, $\epsilon=10^{-8}$.</li>
<li>Gradient norm clipping at 1.0, weight decay 0.1.</li>
</ul>
<h2 id="Dataset">Dataset</h2>
<p><img src="/blogs/joplin_resources/a4987b442d321886f97a47d8c418fe2e.png" alt=""></p>
<p>In total 339B tokens.</p>
<h2 id="Parallelism-in-training">Parallelism in training</h2>
<p>Overall: ZeRO Data parallelism + Megatron-LM Tensor parallelism + 1F1B Pipeline parallelism.</p>
<h3 id="Considerations-on-bandwidth">Considerations on bandwidth</h3>
<ul>
<li>Tensor parallelism has the largest communication overhead of the three strategies, and so we prioritize placing tensor parallel works within a node.</li>
<li>Pipeline parallelism has the lowest communication volume, and so we can schedule pipeline stages across nodes.</li>
<li>Data parallel works are placed within a node to accelerate gradient communications when possible, otherwise are mapped to nearby nodes when possible.</li>
</ul>
<h3 id="Parallelism-degree">Parallelism degree</h3>
<ul>
<li>Each 530B parameter model replica spans 280 NVIDIA A100 GPUs, with 8-way tensor parallelism within a node and 35-way pipeline parallelism across nodes. Data parallelism is used to further scale out to thousands of GPUs.</li>
<li>Model is trained with mixed precision on NVIDIA’s Selene supercomputer with 560 DGX A100 nodes, with each node having 8 NVIDIA 80-GB A100 GPUs (so in total 4480 GPUs). GPUs are connected with NVLink and NVSwitch within node and Infiniband across nodes.</li>
</ul>
<h2 id="Results">Results</h2>
<p>On LAMBADA<br>
<img src="/blogs/joplin_resources/81a78454a41ffe033f20f81605df6ba6.png" alt=""></p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Other papers on DeepSpeed-MoE</title>
    <url>/blogs/2025/01/18/Other-papers-on-DeepSpeed-MoE/</url>
    <content><![CDATA[<h2 id="A-Hybrid-Tensor-Expert-Data-Parallelism-Approach-to-Optimize-Mixture-of-Experts-Training">A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training</h2>
<p>This paper further adopts tensor model parallelism from Megatron-LM with Expert parallelism and ZeRO data parallelism for MoE model training. The design spirit is similar to DeepSpeed-MoE:</p>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/0c9e4c4fd307433aa71d55d75226e244.png" alt=""></p>
<p>In forward pass, two “all-reduce” and two “all-to-all” for each transformer layer. In backward pass, an additional “all-reduce” operator is used to synchronize gradients from different data parallel groups.</p>
<blockquote>
<p>The authors in this paper also proposed a tiled version of optimizer that partitions the parameters into tiles which could be process sequentially, so that the temporary memory for 32-bit gradients is independent of the number of experts and the base model sizes.</p>
</blockquote>
<h2 id="Scaling-Vision-Language-Models-with-Sparse-Mixture-of-Experts">Scaling Vision-Language Models with Sparse Mixture of Experts</h2>
<p>This is an application on VLM.</p>
<h3 id="Architecture">Architecture</h3>
<p>Similar to BeiT v3, but replace the FFN every second layer with the expert layer (only for the first $L-F$ layers, normal FFN in the last $F$ layers).</p>
<p><img src="/blogs/joplin_resources/7993c31955d1c0d7161da317b0d14593.png" alt=""></p>
<p>The first $L-F$ layers are for unimodal inputs, where text or image tokens are fed accordingly to V-MoE or T-MoE. The last $F$ layers have additional VL-FFN that can take data if the input is image-text multimodal input.</p>
<h3 id="Training-objectives">Training objectives</h3>
<p>It uses masked data modeling like BeiT v3.</p>
<ul>
<li>For texts, like BERT, the mask ratio is 15% and the model is trained to recover the masked tokens.</li>
<li>For images, like MAE, using block-wise masking but mask ratio is 40%, the input image is tokenized using the tokenizer in BEiT v2, where tokens are discretized (similar to VQ-VAE).</li>
<li>For image-text pairs, text and image mask ratio keeps the same as MLM and MIM (i.e., 15% and 40% I think), and the masked contents need to be recovered by the model based on multimodal input.</li>
</ul>
<p>Training objectives will also consider the loading loss for experts.</p>
<p>During fine-tuning, all the MoE modules, i.e., routers and experts, are frozed.</p>
<h3 id="Model-configuration-and-size">Model configuration and size</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>#layers</th>
<th>L-F/F</th>
<th>#parameters</th>
<th>Maximum sequence length/Tokenizer</th>
<th>Image resolution/Tokenizer</th>
<th>Batch size</th>
<th>Content in each batch</th>
<th>Training steps/epochs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>12</td>
<td>9/3</td>
<td>2B (180M per token)</td>
<td>128/SentencePiece</td>
<td>224x224/BEiT v2 tokenizer</td>
<td>6144</td>
<td>2048 images, 2048 texts, 2048 image-text pairs</td>
<td>200k steps/40 epochs</td>
</tr>
<tr>
<td>Small</td>
<td>8</td>
<td>7/1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="Dataset-for-pre-training">Dataset for pre-training</h3>
<table>
<thead>
<tr>
<th>Modal</th>
<th>Dataset</th>
<th>Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>English Wikipedia, BookCorpus</td>
<td>4.7B words for EW + 1B words for BC</td>
</tr>
<tr>
<td>Image</td>
<td>ImageNet-22K</td>
<td>14M images</td>
</tr>
<tr>
<td>Text-Image</td>
<td>Conceptual Captions, SBU Captions, COCO, and Visual Genome</td>
<td>4M images and 10M image-text pairs in total</td>
</tr>
</tbody>
</table>
<h3 id="Performance-on-fine-tuning-on-downstream-tasks">Performance on fine-tuning on downstream tasks</h3>
<p><img src="/blogs/joplin_resources/4256c377ad897626662b1871efd811b6.png" alt=""></p>
<p>Above is for VL tasks.</p>
<p><img src="/blogs/joplin_resources/c2c6f4f2d2c6a964974f80c4c5523ca3.png" alt=""></p>
<p>Above is for Vision/Language only downstream tasks. ImageNet has 1.3M images with 1K classes, and MNLI has 433K sentence pairs.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>VLM</tag>
      </tags>
  </entry>
  <entry>
    <title>ZeRO-Infinity</title>
    <url>/blogs/2025/01/13/ZeRO-Infinity/</url>
    <content><![CDATA[<h2 id="ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</h2>
<p>paper (2021 arxiv): <a href="https://arxiv.org/abs/2104.07857">https://arxiv.org/abs/2104.07857</a></p>
<h3 id="The-highlight-of-the-method">The highlight of the method</h3>
<ol>
<li>The much larger models training can scale to with the same number of GPUs compared with prior methods.<br>
<img src="/blogs/joplin_resources/00b9efa88bd0739b7e943fe529a345e2.png" alt=""></li>
<li>High throughput and good scalability<br>
<img src="/blogs/joplin_resources/8a85f10d4d6dc087d5c8c909ec405344.png" alt=""><br>
Surprisingly, even with small model size, ZeRO-Infinity performs comparably with 3D parallelism and ZeRO-Offload.</li>
</ol>
<span id="more"></span>
<h3 id="My-memory-on-ZeRO-Infinity">My memory on ZeRO-Infinity</h3>
<h4 id="Memory-we-have">Memory we have</h4>
<p><img src="/blogs/joplin_resources/c3cedb168b6a6721a4dff204318e3de6.png" alt=""></p>
<p>The above figure shows the memory requirement for different sizes of models, where sequence length is 1024 and batch size is 32 (most likely, as implied in the paper), and the memory bandwidth for communication between different devices (refer to the paper for detailed calculation).<br>
Theoretically, for a single GDX-2 node with 16 GPUs, the aggregate memory on NVMe is 28T, allowing a 1-T model to be trained which requires about 19T memory during training (if activation checkpoints are used).</p>
<h4 id="Requirement-on-memory-bandwidth-for-efficient-training">Requirement on memory bandwidth for efficient training</h4>
<p>The problem is the bandwidth from GPU to CPU/NVMe is slow that would likely to be the bottleneck of throughput. How to fully use the slow memory bandwidth to prevent it being the bottleneck is the core of this paper.</p>
<p>By analyzing the equation of efficiency:<br>
<img src="/blogs/joplin_resources/b148a02b87899d410bd9bf9f5ef1a732.png" alt=""><br>
The $ait$ (arithmetic intensity) can be obtained for the following three parts: model parameters and gradients, optimizer states, and activation memory, respectively, by computing the total computation and data movement per training iteration for the transformer model. The $ait$ is a function of batch size, hidden dimension, and sequence length. After that, we let the $peak_{tp}$ is 70TFlops/GPU based on v100, and we can draw the curves of efficiency w.r.t. batch size, hidden dimension, and bandwidth, assuming the sequence length is 1024 (I am not going to details about how the $ait$ is calculated, please refer to the original paper):</p>
<p><img src="/blogs/joplin_resources/28563fd4e1e63c8ef9529a73538dbf48.png" alt=""></p>
<p>From the above figure, we got three important numbers for the bandwidth that can allow efficient training.</p>
<ul>
<li>For parameters and gradients, the bandwidth requirement is <strong>70GB/s</strong> corresponding to an efficiency of 50%, because for this case, communication can be overlapped with computation, since forward and backward propagation are both executed sequentially. For example, while doing forward propagation for the $i$th operator, the parameters for $(i+1)$th operator can be moved at the same time (i.e., when applying ZeRO-3, move of parameters is actually broadcasting parameters from the host GPU of the parameters to other GPUs). The same mechanism applies to gradients and parameters during backward propagation.</li>
<li>For optimizer states, the authors of the paper state that the communication cannot be overlapped with computation. Therefore, a bandwidth of <strong>1.5TB/s</strong> is required in order to achieve a 90% efficiency.</li>
<li>For activation checkpoint memory, a bandwidth of <strong>2GB/s</strong> is obtained to also achieve a 50% efficiency due to the the possibility of overlapped computation and communication.</li>
</ul>
<blockquote>
<p>Note: ZeRO-Infinity is applied based on ZeRO-3</p>
</blockquote>
<h4 id="How-to-fully-use-memory-to-scale-to-trillion-parameter-models">How to fully use memory to scale to trillion-parameter models?</h4>
<p><img src="/blogs/joplin_resources/aa0c31714a328bfc8c34e6562c281067.png" alt=""></p>
<ul>
<li>Like above, just offload memory to CPU/NVMe (implemented with a proposed <em>infinity offload engin</em> by the authors).</li>
<li>CPU offload for activation, as the memory requirement of activations are small.</li>
<li>Reduce the working memory, by breaking down a large operator into small tiles that can be executed sequentially (the working memory for model state in Figure 2 (a) is calculated based on the largest operator, i.e., the h-&gt;4h linear projection. By using the tiling strategy, the linear operator is represented by a mathematically equivalent sequence of smaller linear operators).</li>
</ul>
<h4 id="How-to-fully-use-slow-memory-bandwidth-to-remain-high-training-efficiency">How to fully use slow memory bandwidth to remain high training efficiency?</h4>
<p><img src="/blogs/joplin_resources/0e5f20e06b0f0057017448b9bedf5413.png" alt=""></p>
<ol>
<li>For parameters and gradients<br>
There are two strategies to enable efficient communication.
<ul>
<li>Now we are using ZeRO-3, in the original way, the parameters (usually of a individual operator) are broadcast from other GPUs whenever needed, during which the parameters located in the CPU or NVMe is first moved to the owner GPU via PCIe, followed by broadcast to other GPUs. This causes many idle PCIes because  only the PCIe connected to the owner GPU is active. To solve that, the parameters of the individual operator is further partitioned into all the data parallel GPUs. In this way, the parameters of even an individual operator has many owner GPUs instead of only a single owner GPU. Therefore, during moving the data from CPU or NVMe to GPU, all the PCIes can be active, achieving an aggregate bandwidth linear to the data parallel degree. For example, with 1024 GPUs used in a data parallelism, the aggregate bandwidth for moving data from CPUs to GPUs can be 3TB/s (1024 * 3GB/s). The process is shown above, applying to both parameters and gradients.</li>
<li>The second is called overlap centric design, which is also quite simple. Basically speaking, when doing the forward pass for the $i$th operator, the parameters of the $(i+1)$th parameters can be at the same time moved from CPUs to GPUs, and the parameters of the $(i+2)$th operator is moved from NVMe to CPUs. Same spirit is applied in backward propagation.</li>
</ul>
</li>
<li>For optimizer states
<ul>
<li>A powerful C++ library called <em>DeepNVMe</em> for asynchronous communication that allows ZeRO-Infinity to overlap NVMe to CPU reads with CPU to NVMe writes, as well as the CPU computation for the optimizer step (bring the data from NVMe to CPU memory and back in chunks that can fit in the CPU memory to perform the optimizer step, one chunk at a time).</li>
<li>Pinned memory management layer that reuses a pinned memory to gradually move the entire optimizer states memory from GPU to CPu or NVMe (pinned memory buffer is important to ensure high-performance tensor reads and writes).</li>
</ul>
</li>
</ol>
<blockquote>
<p>Notes: the authors also propose “Ease inspired implementation” to ease use, please refer to the paper for details at section 7.</p>
</blockquote>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>ZeRO-Offload</title>
    <url>/blogs/2025/01/13/ZeRO-Offload/</url>
    <content><![CDATA[<h2 id="ZeRO-Offload-Democratizing-Billion-Scale-Model-Training">ZeRO-Offload: Democratizing Billion-Scale Model Training</h2>
<p>paper (2021 arxiv): <a href="https://arxiv.org/abs/2101.06840">https://arxiv.org/abs/2101.06840</a></p>
<h3 id="My-memory-on-ZeRO-Offload">My memory on ZeRO-Offload</h3>
<p>ZeRO-Offlad helps multiple GPUs to scale to larger models that have more number of parameters by offloading the part of the memory to CPUs during training, without affecting the efficiency.</p>
<p>Way of thinking:</p>
<ul>
<li>As much memory as possible offloaded to CPUs.</li>
<li>The communication overhead should be as small as possible.</li>
<li>The computation overhead on CPUs shouldn’t affect the training efficiency.</li>
</ul>
<span id="more"></span>
<p>Here is the solution for offloading:<br>
<img src="/blogs/joplin_resources/aef786e78878724e9ee9422bb14d23a5.png" alt=""></p>
<p>Based on that, the authors propose to offload the optimizer (i.e., Adam optimizer) to CPUs where mixed precision training is applied, where 16-bit parameters are located in GPUs, but 16-bit gradients,  32-bit parameters, 32-bit variance, and 32-bit momentum[^1] are located in CPUs. Particularly,</p>
<ol>
<li>For the single-GPU case, after the 16-bit gradients are obtained by backward propagation in GPU, they are offloaded to CPUs and the optimizer uses the gradients for calculating parameter updates. Then the updated 32-bit parameters are converted to 16-bit which are then moved back to GPU.</li>
<li>For the multi-GPU case, where ZeRO-2 is applied, then during the backward propagation, gradients are calculated from back to front. When a certain part of parameters got the gradients, the gradients are averaged by the reduce operator and sent to the corresponding data parallel GPU, which are then offloaded to CPU. After the step function is executed in CPU, the updated parameters are convreted to 16-bit and sent back to the GPU, followed by the all-gather operation or broadcast operation[^2] when parameters are updated and located in GPUs (the actual communication process may differ according to the communication strategy when applying ZeRO-2 and the offload method).</li>
<li>The step function in CPU for the gradients of parameters in later layers and the GPU-&gt;CPU communication can actually be overlapped with backward computation for parameters in previous layers,</li>
<li>For small batch sizes, the communication and computation in CPUs would occupy a big part of the overall computation, which may be a bottleneck of the throughput. Therefore, the authors proposed to delay the CPU offloading by one iteration, to make sure that the CPU computation can be overlapped with the GPU computation.</li>
</ol>
<h3 id="Some-interesting-results">Some interesting results</h3>
<p><img src="/blogs/joplin_resources/e4272620943e493d8fc12b57e790780b.png" alt=""></p>
<p>For Figure 10 where a single DGX-2 node is used (16 GPUs), each experiment uses a fixed total batch of 512 (possibly use gradient accumulation). For 1B-15B models, ZeRO-Offload achiever higher throughput than any model because it allows to train with larger batch sizes since the GPU memory is partially offloaded to CPUs. ZeRO-Offload with model parallelism can scale to a 70B-parameter model.</p>
<p><img src="/blogs/joplin_resources/0eba1fd3580766b70a9f22f3acadd7e2.png" alt=""></p>
<p>For Figure 11, with more GPUs (such as 64 and 128), ZeRO-2 starts to outperform ZeRO-Offload, because both can now run similar batch sizes, achieving similar computation efficiency, whereas ZeRO-2 does not suffer from the additional overhead of CPU-GPU communication.</p>
<p>[^1] In mixed-precision training, Adam optimizer state has 32-bit parameters, variance, and momentum, while the model itself has the 16-bit parameters and gradients.<br>
[^2] For all-gather operation, the the CPUs will execute the step function in parallel when the whole backward propagation is finished and all the gradients are offloaded to CPUs. Then the updated parameters in optimizer states will be moved back to the corresponding GPUs followed by this all-gather operation. For broadcast operation, the CPUs execute step function sequentially once a certain part of gradients are calculated and offloaded to the CPUs. Similarly, the updated parameters will be moved back to the corresponding GPU. Since these parameters only in a single GPU now (in ZeRO-2 data parallelism), they are broadcast to other GPUs.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Flash Attention 1 &amp; 2</title>
    <url>/blogs/2025/01/20/Flash-Attention-1-2/</url>
    <content><![CDATA[<h1>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h1>
<p>paper (2022 arxiv): <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></p>
<h2 id="Background-compute-bound-and-memory-bound">Background: compute-bound and memory-bound</h2>
<ul>
<li>The performance on throughput of transformer layers can be either compute-bound or memory-bound. The higher the arithmetic intensity, the more likely to be memory-bound.</li>
<li>Compute-bound operators include matrix multiplication where the computation takes more time than communication or data movement, memory-bound operators perform in an opposite way, including element-wise operators, and reduction, e.g., sum, softmax, batch norm, etc.</li>
</ul>
<span id="more"></span>
<h3 id="Equation-of-attention">Equation of attention</h3>
<p>Here I just take a snip from the paper:</p>
<p><img src="/blogs/joplin_resources/05003393e95ed1d262dd674027b277fc.png" alt=""></p>
<h3 id="Standard-attention-implementation-is-memory-bound">Standard attention implementation is memory-bound</h3>
<p>It is memory-bound due to the quadratic complexity of memory reads/writes (or data movement) to the sequence length, i.e., $O(N^2)$, due to the softmax function and other element-wise operations applied to the attention matrix like masking, dropout.</p>
<p>In addition, GPU has hierarchy memory structure that comprises multiple forms of memory of different sizes and speeds:<br>
<img src="/blogs/joplin_resources/86cc4118282213911c4965f64c09dfd2.png" alt=""></p>
<p>We could see that SRAM has much higher bandwidth than HBM (high bandwidth memory), but unfortunately, the standard implementation involves the $O(N^2)$ data movement from and to HBM, becoming a bottleneck of performance:</p>
<p><img src="/blogs/joplin_resources/876688f56d9a770a95df6f545979003f.png" alt=""></p>
<blockquote>
<p>Note: from my personal view, seems like the statement of the issue is problematic since we could simply don’t write $S$ to HBM and continue using it to computer $P$. In line 3, we can also avoid writing $P$ to HBM and directly use it with $V$ loaded from HBM to computer $O$.<br>
But the authors also say that GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM. So it is because the computation of attention is divided into multiple kernels, e.g., $P=softmax(S)$, so these individual kernels should read data from HBM, and write the result back to HBM for the second kernel to read.</p>
</blockquote>
<h3 id="Solution">Solution?</h3>
<p>Obviously, we can <strong>fuse these kernels into one kernel</strong> to be executed in SRAM without reading $S$ from and writing $P$ to HBM. This is actually what FlashAttention is doing.</p>
<h2 id="FlashAttention-via-tiling-and-recomputation">FlashAttention via tiling and recomputation</h2>
<p>Here is the algorithm for the forward implementation:<br>
<img src="/blogs/joplin_resources/c9a06b8042a690d65efd34cedffcf1d3.png" alt=""></p>
<p>Basically, the tricks that divide $Q$ into $T_r$ segments along the rows, and divide $K$ and $V$ into $T_c$ segments along the columns is called <code>tiling</code>. So after tiling, the computation of attention layer is divided into $T_r\times T_c$ small pieces, for each piece the SRAM could locate all the intermediate results to finish the computation. We use $l\in \mathbb{R}^N$ to record the updated softmax normalization factor, and $m\in \mathbb{R}^N$ to record the maximum elements in rows. Because the the inner loop is long the columns of $K$, so $l$ and $m$ will only be finally determined after the whole inner loop.</p>
<p>The use of $m$, I guess, is to maintain the numerical stability to avoid huge numbers that can cause overflow.</p>
<p>If you still don’t understand the tiling, here is the original introduction:<br>
<img src="/blogs/joplin_resources/9cb4d32da27f092ae9acb25a1b87b758.png" alt=""><br>
where, we have to track $l$ and $m$ during tiling.</p>
<p>Actually, the above algorithm skips dropout, causal mask, and scaling of $QK^T$ by $1/\sqrt(d)$. But the addition of these operations is straightforward.</p>
<p>For backward implementation, it follows the similar spirit. One difference is that it additionally uses gradient checkpointing (activation <code>recomputation</code>) for the attention map, and the dropout mask can be also recomputed known the pseudo-random number generator in the forward pass. Please refer to the appendix in the original paper.</p>
<h3 id="Complexity-of-communication-or-HBM-accesses">Complexity of communication (or HBM accesses)</h3>
<p>Here, since FlashAttention prevent the movement of the data with size $N^2$, by combing all the movement from and to HBM, the communication complexity (number of HBM accesses) is proved to be $o(N<sup>2d</sup>2M^{-1})$.</p>
<h3 id="Block-sparse-FlashAttention">Block-sparse FlashAttention</h3>
<p>This is quite simple, since some techniques can be used to sparsify the attention maps block-wise to speed up inference, so we could safely skip all the spasified blocks out of the total $T_r\times T_c$ blocks.</p>
<h2 id="Results">Results</h2>
<p><img src="/blogs/joplin_resources/4eb0a485640619986ac3299e883a5e3b.png" alt=""><br>
Above figure shows that for context length with 1K, FlashAttention gives higher speedup. And:</p>
<p><img src="/blogs/joplin_resources/f082d5a6fd10e7a791b035c81841516e.png" alt=""></p>
<h2 id="Conclusion">Conclusion:</h2>
<p>IO is important, for memory-bound operations.</p>
<h1>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h1>
<h2 id="Background">Background</h2>
<p>The non-matmul FLOPs are now becoming a bottleneck since its throughput is $16\times$ slower than that of the matmul FLOPs. This is because matmul operations are highly optimized in GPUs.</p>
<p>So the solution is to reduce the number of non-matmul FLOPs in FlashAttention.</p>
<h2 id="FlashAttention-2">FlashAttention-2</h2>
<p>Assuming $T_r=2$ and $T_c=2$, the tiling of computation can be expressed as:<br>
<img src="/blogs/joplin_resources/07027be4807a999525fabdba0c4cf6c4.png" alt=""></p>
<p>Then we found that<br>
<img src="/blogs/joplin_resources/a901af6682d5eec4cc59c463a0251a87.png" alt=""><br>
and (the below is for backpropagation)<br>
<img src="/blogs/joplin_resources/c1bd17415d2b700587bd540f304a6c05.png" alt=""></p>
<p>So the algorithm of forward propagation now becomes:<br>
<img src="/blogs/joplin_resources/6d5909408863e2435a0ed74a32423071.png" alt=""></p>
<p>The algorithm for backward propagation follows the same spirit.</p>
<h3 id="Another-finding-in-the-algorithm">Another finding in the algorithm</h3>
<p>We could see another big difference is that the order of loops is switched compared to original FlashAttention. Here, the outer loop is along the rows of $Q$, which means each iteration in the outer loop can be executed independently without communicating with each other. In other words, the outer loops can be executed in parallel. But the backward propagation should be further taken care of since it involves some communication between outer loop executions.</p>
<h2 id="Parallelism">Parallelism</h2>
<p>The original FlashAttention can be paralleled alongside data (or batch) and head. Based on the above finding, FlashAttention-2 can be further paralleled alongside its outer loop, i.e., the rows of $Q$, or the sequence.</p>
<p>For backward propagation, we also parallelize the computation along sequence but along the columns of $K$ and $V$, and use atomic adds to communicate between different workers.</p>
<p>The additional parallelism dimension allows us to better use of multiprocessors on the GPU to improve the occupancy of GPUs.</p>
<h2 id="Results-v2">Results</h2>
<p><img src="/blogs/joplin_resources/edb85315cd62f418b02a5d17ef5544ed.png" alt=""></p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Distributed training</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>InstructGPT-RLHF</title>
    <url>/blogs/2024/06/27/InstructGPT-RLHF/</url>
    <content><![CDATA[<h2 id="Training-language-models-to-follow-instructions-with-human-feedback">Training language models to follow instructions with human feedback</h2>
<span id="more"></span>
<p><img src="/blogs/joplin_resources/ed41c95657c09fd229c5ed16ef514980.png" alt=""></p>
<p>The motivation is that the output of GPT series are not aligned with the user needs, since the pre-text task in GPT is to predict next word, but the expectation of ChatGPT is to be useful and helpful for users. Thus, the authors give a 3-stage framework to fine-tune GPT-3.</p>
<p>First, they collect some prompts (questions) and answers from users/labelers, and use these to fine-tune GPT-3.</p>
<p>Second, they ask labelers to rank the multiple outputs of the model from step 1 for a single prompt. Then they train a reward model to match the ranking.</p>
<p>Third, they further train the model from step 1 such that its output has a high score from the reward model from step 2.</p>
<p>It is called reinforcement learning from human feedback, or RLHF.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek Series - VLM</title>
    <url>/blogs/2025/04/01/DeepSeek-Series-VLM/</url>
    <content><![CDATA[<p>Paper list</p>
<ul>
<li>DeepSeek-VL: Towards Real-World Vision-Language Understanding</li>
<li>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</li>
<li>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</li>
<li>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</li>
<li>Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling</li>
</ul>
<h2 id="General-information">General information</h2>
<p>DeepSeek-VL and VL2 are two multimodal models for understanding, no visual generation.<br>
Janus series are text and image generative models. Among them, Janus and Janus-Pro use autoregressive mechanism for the image generation, while JanusFlow use Rectified flow, like the diffusion models, to iteratively refine the generated contents from a noise to an image.</p>
<span id="more"></span>
<h2 id="DeepSeek-VL-Towards-Real-World-Vision-Language-Understanding">DeepSeek-VL: Towards Real-World Vision-Language Understanding</h2>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>A diverse range of data source</td>
<td>1.3B, 7B</td>
<td>Adapter+ Joint training + Instruction tuning</td>
<td>Comparative to state-of-the-art</td>
</tr>
</tbody>
</table>
<h3 id="Overview">Overview</h3>
<ol>
<li>Open-source, it’s the most important one for the first DeepSeek paper (here, of the VLM series).</li>
<li>Deal with both high and low image resolutions using two encoders;</li>
<li>Preserve language capability via a modality warm-up strategy.</li>
</ol>
<h3 id="Method">Method</h3>
<h4 id="Data">Data</h4>
<p>In a few words, they encompass a diverse range of publicly accessible sources, in addition to a selection of proprietary data. They also collect a bunch of supervised fine-tuning data from a diverse range of multimodality and language data sources.</p>
<h4 id="Architecutre">Architecutre</h4>
<p>So the architecture consists of three parts:</p>
<ul>
<li>The hybrid vision encoder.
<ul>
<li>SAM-B, a pretrained ViTDet encoder that accepts $1024\times 1024$ image inputs.</li>
<li>SigLIP-L encoder for $384\times 384$ image inputs.<br>
With these two encoders, the image’s semantic and detailed information are both preserved.</li>
</ul>
</li>
<li>Vision-Language adaptor<br>
The input to the adaptor is the concatenated output from the two vision encoders. The adaptor transforms the input into the LLM’s input space through some MLPs.</li>
<li>Language model<br>
Use DeepSeek LLM, so the model is initialized from a selected checkpoint of DeepSeek LLM’s pretrained model.</li>
</ul>
<h4 id="Training-pipeline">Training pipeline</h4>
<p><img src="/blogs/joplin_resources/73979f1d7ac8563c66ac3620e12fc9c0.png" alt=""></p>
<p>The training objective is the next token prediction.<br>
Like above, on stage 1 they train the adaptor to establish a conceptual link between visual and linguistic elements within the embedding space. On stage 2 jointly train the adaptor and LLM and use a ratio of roughly 7:3 of language to multimodal data, to enable the model to maintain its language capabilities while achieves better pretraining on multimodal data. On stage 3 they conduct supervised fine-tuning and train all parameters.</p>
<p>DeepSeek-VL-7B consumed 5 days on a cluster of 64 nodes, each comprising 8 Nvidia A100 GPUs, while DeepSeek-VL-1B consumed 7 days on a setup involving 16 nodes. Here is the performance:<br>
<img src="/blogs/joplin_resources/ce9c596abb969eb4d41bc3787a22df09.png" alt=""></p>
<h2 id="DeepSeek-VL2-Mixture-of-Experts-Vision-Language-Models-for-Advanced-Multimodal-Understanding">DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</h2>
<p><img src="/blogs/joplin_resources/de88485afccd6d3e78a249c1baf1c44f.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>An improved data</td>
<td>3B / 0.57B activated  <br>16B / 2.4B activated  <br>27B / 4.1B activated</td>
<td>Alignment+ Joint training + Instruction tuning</td>
<td>Shown above</td>
</tr>
</tbody>
</table>
<h3 id="Overview-v2">Overview</h3>
<ol>
<li>Using dynamic tiling to tackle any resolutions;</li>
<li>Okay, now we have the DeepSeekMoE architecture.</li>
<li>Maybe better data for training and some change on the training pipeline.</li>
</ol>
<h3 id="Method-v2">Method</h3>
<p><img src="/blogs/joplin_resources/1f059e7f7781c9e3dc1eb7c0ed3a8c5f.png" alt=""></p>
<p>The above figure shows the overall framework. So, first the dynamic tilling approach. It’s quite simple, as shown below.<br>
<img src="/blogs/joplin_resources/b67ecfc3fa09e661dcb6fb770b55738e.png" alt=""><br>
We basically use a single SigLIP-SO400M-384 vision encoder and divide the original image into many $384\times 384$ tiles and do the encoding.</p>
<p>Then the architecture adopts the MoE version, and the pretraining data contains both multimodal data and pure language data to preserve the language capabilities. The rest of the others are quite similar to VL. Note that for the training pipeline, the stage 1 will optimize both the adaptor and the vision encoder.</p>
<p>That’s all to mention here.</p>
<h2 id="Janus-Decoupling-Visual-Encoding-for-Unified-Multimodal-Understanding-and-Generation">Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</h2>
<p><img src="/blogs/joplin_resources/2286717cf3b6f9adcd8733210ff36604.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>DeepSeek LLM (1.3B)</td>
<td>3-stage training</td>
<td>Shown above</td>
</tr>
</tbody>
</table>
<h3 id="Overview-v3">Overview</h3>
<p><img src="/blogs/joplin_resources/a7f033d53bc0a574cfaa6e6a8cf15c15.png" alt=""><br>
The main ideas:</p>
<ol>
<li>Entire model adheres to an autoregressive framework;</li>
<li>Janus decouples visual encoding for visual understanding and visual generation. I.e., it uses two vision encoders and two adaptors.</li>
</ol>
<h3 id="Method-v3">Method</h3>
<h4 id="Vision-encoders-and-adaptors">Vision encoders and adaptors</h4>
<table>
<thead>
<tr>
<th></th>
<th>Encoder and adaptor</th>
</tr>
</thead>
<tbody>
<tr>
<td>Understanding</td>
<td>SigLIP, the feature maps are flattened to 1-D + an adaptor to map the feature to LLM input space</td>
</tr>
<tr>
<td>Generation</td>
<td>A VQ tokenizer, similar mapping to LLM input space</td>
</tr>
</tbody>
</table>
<p>The output of the two adaptor are concatenated and fed to the LLM.</p>
<h4 id="Training-pipeline-v2">Training pipeline</h4>
<p><img src="/blogs/joplin_resources/bc3b6e04420368e61d8cd5f36f21fec0.png" alt=""></p>
<p>For stage 1, the dataset includes 1.25M image-text paired captions from ShareGPT4V, and 1.2M images from ImageNet-1K where the text is organized as ‘&lt;category_name&gt;&lt;image&gt;’.<br>
For stage 2, they use 1) text-only data, 2) interleaved image-text data, and 3) image caption data, 4) table and chart data, and 5) visual generation data.<br>
For stage 3, they use instruct tuning data.<br>
Flame symbols/snowflake symbols in the diagram indicate the module updates/does not update its parameters.</p>
<p>Maybe that’s all for the paper.</p>
<h2 id="Janus-Pro-Unified-Multimodal-Understanding-and-Generation-with-Data-and-Model-Scaling">Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling</h2>
<p><img src="/blogs/joplin_resources/6796c88e04a50246b5d52f8e31240b13.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>1B, 7B</td>
<td>3-stage training</td>
<td>Shown above</td>
</tr>
</tbody>
</table>
<h3 id="Overall">Overall</h3>
<p>Compared with Janus, the Pro version incorporates:</p>
<ol>
<li>an optimized training strategy,</li>
<li>expanded training data,</li>
<li>scaling to larger model size.</li>
</ol>
<h3 id="Method-v4">Method</h3>
<p>So for the training pipeline, I’ll directly take a snip from the paper to tell the main modification:<br>
<img src="/blogs/joplin_resources/6ac4d9f4433d604aaa8910bc6da88cdf.png" alt=""></p>
<p>Besides, they also scale the data and model size for better performance.</p>
<p>Maybe that’s all.</p>
<h2 id="JanusFlow-Harmonizing-Autoregression-and-Rectified-Flow-for-Unified-Multimodal-Understanding-and-Generation">JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</h2>
<p><img src="/blogs/joplin_resources/41982dd75b030bdc77b72a85fb1cc939.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>1B, 7B</td>
<td>3-stage training</td>
<td>Shown above</td>
</tr>
</tbody>
</table>
<h3 id="Overview-and-Method">Overview and Method</h3>
<p>It’s kind of another branch of Janus but using diffusion technique to generate images.</p>
<p>Instead of the original diffusion process, they use rectified flow to iteratively transform a noise to a sample in the image distribution. The training objective is:<br>
<img src="/blogs/joplin_resources/5c9c082e6e6a8efd4ba1e870b028b383.png" alt=""><br>
where $z_0$ is a sample from the standard Gaussian distribution, $v_{\theta NN}$ is a neutral network taking the point in the linear path from $z_0$ to $x$ and the time step $t$ as input, to predict the difference between $x$ and $z_0$.<br>
If you don’t understand, Sec. 3.1 in this paper gives some good introduction then.<br>
Based on that, the framework of JanusFlow is like:<br>
<img src="/blogs/joplin_resources/7141e4d05e6b33a941adabf585df866a.png" alt=""></p>
<p>The figure below gives a good illustration of the 3-stage training:<br>
<img src="/blogs/joplin_resources/e5400cbbb9bec63deaceb2cf0910ab41.png" alt=""></p>
<p>Three training objectives are used.</p>
<ul>
<li>Autoregression Objective<br>
<img src="/blogs/joplin_resources/06cfce6af84f521c6f12dc00089972dc.png" alt=""></li>
<li>Rectified Flow Objective<br>
<img src="/blogs/joplin_resources/d94d5a43e525310ded726eaacff04a8b.png" alt=""></li>
<li>Representation Alignment Regularization<br>
<img src="/blogs/joplin_resources/f0103f380a96b1652a1c0d0923fa1deb.png" alt=""></li>
</ul>
<p>02/04/2025 01:19<br>
Maybe I need to go to sleep now.</p>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>VLM</tag>
        <tag>DeepSeek</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek Series - LLM</title>
    <url>/blogs/2025/03/30/DeepSeek-Series-LLM/</url>
    <content><![CDATA[<p>Paper list (papers in gray is not discussed in this blog)</p>
<ul>
<li>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</li>
<li>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</li>
<li>DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</li>
<li>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</li>
<li><span style="color: gray;">DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data</span></li>
<li><span style="color: gray;">DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</span></li>
<li>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</li>
<li><span style="color: gray;">Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</span></li>
<li><span style="color: gray;">DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search</span></li>
<li>DeepSeek-V3 Technical Report</li>
<li>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</li>
</ul>
<h2 id="General-information">General information</h2>
<p>DeepSeek is a series of strong LLM models, open-source but trying to beat close-source models, in terms of general language generation, and specific fields like Math, coding, reasoning, etc. The famous powerful DeepSeek V2/V3/R1 use MoE architectures, a huge amount of training tokens, and novel architectural, training, and optimization innovations, to get state-of-the-art performances with efficient training and inferences. In addition, DeepSeek also developed a series powerful Vision-Language models, which will be discussed in the next blog.</p>
<span id="more"></span>
<p>The details are in the original papers, but let’s try to briefly introduce them here.</p>
<blockquote>
<p>Note: no DeepSeek models, ChatGPT, or other similar LLM tools are used in the blogs.</p>
</blockquote>
<h2 id="DeepSeek-LLM-Scaling-Open-Source-Language-Models-with-Longtermism">DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</h2>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>2T</td>
<td>7B, 67B</td>
<td>Pretraining + SFT + DPO</td>
<td>DeepSeek LLM 67B &gt; LLaMA-2 70B  <br><br/>After SFT + DPO:DeepSeek LLM 67B &gt; GPT-3.5</td>
</tr>
</tbody>
</table>
<h3 id="Overview">Overview</h3>
<p>This might be the first LLM model developed by DeepSeek team. It is a dense LLM that has two versions: 7B and 67B. The main contributions (my understanding) are:</p>
<ol>
<li>Open-source (this is the key for long-term influence);</li>
<li>Re-investigate the scaling law, finding the formulas of the best hyperparameters (learning rate and batch size), non-embedding FLOPs/token, and number of training tokens.</li>
<li>Finding that the data quality is a key factor affecting the model performance. Higher quality data may need more compute budge allocated to model rather than data.</li>
</ol>
<h3 id="Method">Method</h3>
<p><strong>Data</strong>: Using deduplication, filtering, and remixing to create high-quality training data;<br>
<strong>Architecture</strong>: Largely follows the design of LLaMA but applies deeper layers.<br>
<strong>Scaling law</strong>: The best hyperparameters are found by grid search using small scale models, and M, D are found by IsoFLOPs profile approach, where for a fixed compute budge, different M/D scale allocations are designed to draw the curve.</p>
<h2 id="DeepSeekMoE-Towards-Ultimate-Expert-Specialization-in-Mixture-of-Experts-Language-Models">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</h2>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>2T</td>
<td>2B / 0.3B activated  <br><br/>16B / 2.8B activated  <br><br/>145B / 22.2B activated</td>
<td>Pretraining + SFT (for 16B)</td>
<td>DeepSeekMoE 2B ~ GShard 2.9B (the latter has 1.5x expert parameters)  <br><br/>DeepSeekMoE 2B ~ its dense counterpart  <br><br/>DeepSeekMoE 16B ~ LLaMA2 7B (the former has only 40% computations)  <br><br/>DeepSeekMoE 145B ~ DeepSeek 67B (the former has only 28.5% computations)</td>
</tr>
</tbody>
</table>
<h3 id="Overview-v2">Overview</h3>
<p>This paper mainly introduces two ideas for the architecture: fine-grained expert segmentation and shared expert isolation. In addition, some balance loss functions are designed to encourage load balance.</p>
<p><img src="/blogs/joplin_resources/e4a3bb2661d2ad7e8e8a566af5bd67d7.png" alt=""></p>
<h3 id="Method-v2">Method</h3>
<h4 id="Fine-grained-Expert-Segmentation">Fine-grained Expert Segmentation</h4>
<p>As shown in the above figure, based on the basic MoE architecture in (a), it further segments each FFN expert into $m$ smaller experts, such that the number of experts are increased by $m$. But the ratio of the number of activated experts to the total number of experts keeps the same. It means the number of activated experts are also increased by $m$ but are finer-grained. It increase the level of expert specialization.</p>
<h4 id="Shared-Expert-Isolation">Shared Expert Isolation</h4>
<p>It has some shared experts that are activated all the time. The shared experts are dedicated to capturing and consolidating common knowledge across varying contexts, the parameter redundancy among other routed experts will be alleviated.</p>
<h4 id="Load-balance-loss-functions">Load balance loss functions</h4>
<ul>
<li>
<p>Expert-Level Balance loss:<br>
<img src="/blogs/joplin_resources/1045513e17305d61e17e3bd64e6dec2d.png" alt=""><br>
where $N’$ is the number of routed experts (those who are not shared), $K’$ is the number of activated experts, $T$ is the sequence length, $s_{i, t}$ is the token-to-expert affinity (score of $t$th token on $i$th expert output by softmax). Basically, $f_i$ is the actual number of tokens assigned to expert $i$, while $P_i$ is the soft number of tokens assigned to expert $i$. The loss encourage the tokens are evenly assigned across the experts.</p>
</li>
<li>
<p>Device-level Balance loss:<br>
<img src="/blogs/joplin_resources/4e417656419e1ac742d747388a3cd617.png" alt=""></p>
</li>
</ul>
<p>It shows similar spirit. So they partition all routed experts into $D$ groups ${\epsilon_1, \epsilon_2, …, \epsilon_D}$, and deploy each group on a single device. The above is the loss function.</p>
<h2 id="DeepSeek-Coder-When-the-Large-Language-Model-Meets-Programming-–-The-Rise-of-Code-Intelligence">DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</h2>
<p><img src="/blogs/joplin_resources/d97cf075b2124bbb04f1c581db763156.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>2T tokens sourced from 87 programming languages</td>
<td>DeepSeek-Coder 1.3B, 6.7B, 33B (v1)</td>
<td>Pretraining,   <br>Instruction tuning</td>
<td>As shown above</td>
</tr>
<tr>
<td></td>
<td>DeepSeek-Coder-v1.5 7B</td>
<td>Pretraining starting from DeepSeek-LLM-7B Base,  <br>only using next token prediction</td>
<td>Comparing with the v1 models, can increase the performance of math reasoning and natural language categories, with minor degradation on programming.</td>
</tr>
</tbody>
</table>
<p>So, the innovations in this paper are:</p>
<ol>
<li>For training, they also use Fill-In-Middle approach in addition to next token prediction. This method aims to incorporate a fill-in-the-blank pretraining task during the training process. Within the FIM methodology, two distinct modes are employed: PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle). So it means the middle is predicted given both suffix and prefix. It can enhance model’s capability to handle various structural arrangements in code</li>
<li>They incorporate repository-level data construction instead of file-level. Which means they consider all the files in a project and reorder them to ensure the correct dependencies across files. It can potentially increase the practicality and applicability of the model in handling project-level code scenarios.</li>
</ol>
<p> </p>
<h2 id="DeepSeekMath-Pushing-the-Limits-of-Mathematical-Reasoning-in-Open-Language-Models">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</h2>
<p><img src="/blogs/joplin_resources/9ba638166577af14e90c0b92d4a47f6c.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>120B</td>
<td>DeepSeekMath-Base 7B</td>
<td>Additional pretraining from DeepSeek-Coder-Base-v1.5 7B,  <br>mathematical instruction tuning,  <br>GRPO (Group Relative Policy Optimization, a proposed RL algorithm in the paper)</td>
<td>Shown above</td>
</tr>
</tbody>
</table>
<p>Basically, it continues training DeepSeek-Coder-Base-v1.5 7B, with 120B math-related tokens, using mathematical instruction tuning and GRPO.</p>
<h2 id="DeepSeek-V2-A-Strong-Economical-and-Efficient-Mixture-of-Experts-Language-Model">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</h2>
<p><img src="/blogs/joplin_resources/25ca2dbfe9703db53c94864c8dc691ca.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.1T</td>
<td>236B / 21B activated (context length 128K tokens)</td>
<td>Pretraining + SFT + RL (GRPO)</td>
<td>Comparison with DeepSeek 67B is shown above (right), with others (left)</td>
</tr>
</tbody>
</table>
<h3 id="Overview-v3">Overview</h3>
<p>The main idea proposed in this paper is the Multi-head Latent Attention (MLA), in order to reduce the KV cache to save memory and computation. DeepSeek v2 is built based on DeepSeekMoE (using fine-grained expert segmentation and shared expert isolation), using MLA, and introducing extra load balance loss functions, with larger data scale and additional training strategies to reach its goal.</p>
<blockquote>
<p>Question: what is the key for the shocking performance of DeepSeek v2?<br>
I guess from the architectural side, the MLA make DeepSeek v2 efficient in both memory and computation, leading to much less training and inference cost. DeepSeekMoE also provides a good start for effective MoE architecture design considering efficiency and accuracy. From the training perspective, the performance may also comes from some balance loss functions and training pipelines. From the data perspective, 8.1T tokens for pretraining should be definitely playing a role.</p>
</blockquote>
<h3 id="Method-v3">Method</h3>
<h4 id="Multi-head-Latent-Attention-MLA">Multi-head Latent Attention (MLA)</h4>
<p><img src="/blogs/joplin_resources/f33a2c623ef9ddc06b63864f9e088d1c.png" alt=""></p>
<p>Since we already has discussed the DeepSeekMoE architecture (in this blog) and KV cache (in another blog), so maybe let’s quickly talk about this MLA approach that aims to further save memory and computations based on the KV cache strategy.</p>
<p>As shown on the bottom right of the above figure, the spirit is from low-rank compression. The input $h_t$ is first mapped to a feature in a latent space $c_t^{KV}$ where the dimension is small before creating K and V. Then K and V are created based on this latent space by using separate mapping matrix to project the dimension to a larger one. By doing so, only the latent feature is cached which saves a lot of memory and computation. To further speed up the training process, Q is also compressed using the same strategy. The below equation is an example for Q.</p>
<p><img src="/blogs/joplin_resources/fe798aebaec87e632dec6f34c0153708.png" alt=""></p>
<p>For the positional embedding, the rotary position embedding (RoPE) is used and cannot be integrated into the above process (two linear projections) since it is nonlinear. Therefore, the authors propose to decouple that like this (taking Q as an example):<br>
<img src="/blogs/joplin_resources/ddee053120ce99b313fef78cd669415d.png" alt=""><br>
where, $q^R$ is divided into multi heads, but $k^R$ is shared.</p>
<h4 id="Auxiliary-Loss-for-Load-Balance">Auxiliary Loss for Load Balance</h4>
<p>In addition to the two load balance loss functions introduced in DeepSeekMoE, the v2 version further considers a balance loss, namely, communication balance loss:<br>
<img src="/blogs/joplin_resources/056b37fbdb9b420bbeb57d3a68be7a55.png" alt=""><br>
This is to encourage each device receives an equal number of tokens.</p>
<h4 id="Long-Context-Extension">Long Context Extension</h4>
<p>YaRN method is applied on RoPE to extend the length to 160K (so that the performance on 128K should be expected well).</p>
<h2 id="DeepSeek-V3-Technical-Report">DeepSeek-V3 Technical Report</h2>
<p><img src="/blogs/joplin_resources/d649ac45f5554ebdc4b392abb6efa62b.png" alt=""></p>
<table>
<thead>
<tr>
<th>Number of tokens for pretraining</th>
<th>Model size options</th>
<th>Training pipeline</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>14.8T</td>
<td>671B / 37B activated (context length 128K tokens)</td>
<td>Pretraining + SFT + RL (GRPO)<br>The post-training (SFT, RL) data is curated from DeepSeek-R1, so it means using distillation from R1</td>
<td>Above figure</td>
</tr>
</tbody>
</table>
<h3 id="Overview-v4">Overview</h3>
<p>Main ideas include:<br>
1. Auxiliary loss free strategy to ensure load balance (no need to design balance loss function anymore);<br>
2. Multi-token prediction training objective;<br>
3. DualPipe pipeline parallelism strategy;<br>
4. Other training and inference optimization strategies like FP8 training.</p>
<blockquote>
<p>Why v3 stronger?<br>
Definitely, the number of training tokens is increased significantly, from 8.1T in V2 to 14.8T in V3. Larger model scale also gives enhancement. In other words, scaling law.</p>
</blockquote>
<p>Let’s also take a look at the training cost of V3 (I don’t know other LLMs but I guess V3 is much cheaper right?):<br>
<img src="/blogs/joplin_resources/9ee3898969e7193667cf725069db74b3.png" alt=""></p>
<h3 id="Method-v4">Method</h3>
<p>The architecture still continues with the DeepSeekMoE architecture and MLA strategy for KV cache. So here just introduce the new ideas.</p>
<h4 id="Auxiliary-Loss-Free-Load-Balancing">Auxiliary-Loss-Free Load Balancing</h4>
<p><img src="/blogs/joplin_resources/f62603c950185ee0efd3c5185b11528a.png" alt=""></p>
<p>The motivation is the balance loss will make the model less focused on the loss regarding the accuracy. Here as shown in the equation, $b_i$ is a bias term added to the affinity score (kind of reducing the effect of the affinity scores on the loading). During training, the bias term is decreased if the corresponding expert is overloaded, otherwise increased.</p>
<p>But actually, the load balancing is not completely loss free, as the authors also introduce a complementary Sequence-wise loss:<br>
<img src="/blogs/joplin_resources/4255fbbbe4bdbaafb5a6d87a81bc7864.png" alt=""><br>
to prevent extreme imbalance within any single sequence.</p>
<h4 id="Multi-Token-Prediction">Multi-Token Prediction</h4>
<p><img src="/blogs/joplin_resources/72cd5aa9b4cdf445c90599a165268c61.png" alt=""><br>
As shown above, they use additional $D$ MTP modules to predict D additional tokens, keeping the causal chain. The embedding layer and output head is shared in those MTP modules with the main model. The input of the RMSNorm in the MTP module is from  the previous modules or the main model.</p>
<p>During training, each MTP module predict some tokens, giving a loss, and all the losses from the main model and MTP modules are combined with some weights. It also means certain tokens are predicted more than once.</p>
<p>During inference, only the main model is used.</p>
<h4 id="Other-strategies">Other strategies</h4>
<p><img src="/blogs/joplin_resources/cd42d666dd2b105257567164e3c25ada.png" alt=""></p>
<p>The DualPipe is a little complicated, you may need to refer to the paper or code for details. Overall, it carefully divides the structure into smaller segments for each pipeline, and overlaps the communication and computation (to make them happen at the same time) to largely enhance efficiency, and at the same time, to ensure minimum pipeline bubbles.</p>
<p>For other strategies like FP8 mixed precision training, efficient all-to-all communication, recomputation of certain activations (RMSNorm and MLA Up-Projection), and some inference and deployment tricks, please refer to the original paper.</p>
<h2 id="DeepSeek-R1-Incentivizing-Reasoning-Capability-in-LLMs-via-Reinforcement-Learning">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h2>
<p><img src="/blogs/joplin_resources/df6a79e7199fad48c8653f2ff57cc566.png" alt=""><br>
It’s the reasoning performance of different LLMs.</p>
<p>DeepSeek-R1 is based on DeepSeek-V3-Base, using a novel pipeline to develop with hundreds of sounds of samples to do RL.</p>
<h3 id="Overview-v5">Overview</h3>
<ol>
<li>They first propose DeepSeek-R1-Zero, where no supervised data is provided. The finetuning data is collected from the base model itself. Zero already shows great reasoning performance compared with other LLMs (e.g., OpenAI-o1-mini).</li>
<li>Then they propose a novel pipeline to further develop DeepSeek-R1.</li>
<li>They use the samples curated with R1 to finetune other open-source LLMs like Qwen and Llama. They only use SFT (no RL) to finetune these LLMs, even though they demonstrate that applying RL can further enhance the reasoning performance.</li>
</ol>
<h4 id="Method-v5">Method</h4>
<h4 id="DeepSeek-R1-Zero">DeepSeek-R1-Zero</h4>
<p><img src="/blogs/joplin_resources/c4b047bf40e48d5bbdf14347019eed7c.png" alt=""><br>
Zero is based on a DeepSeek base model (not sure which model it refers to in the original paper) and is obtained by using RL, particularly, GRPO (Group Relative Policy Optimization) proposed by the team before. The training data is gathered using the base model by guiding it to adhere to some specific instructions for the output, as shown in the template above.</p>
<p>The reward for RL is from two sides. One is the accuracy rewards, which can be obtained by comparing the output of the model and the ground truth answer, like math, or code questions (using a compiler to check whether the generated code can pass). The other is the format rewards, enforcing the model to output a thinking process (enforces the model to put its thinking process between ‘&lt;think&gt;’ and ‘&lt;/think&gt;’ tags.)</p>
<p>During training, Zero shows increasing reasoning abilities.<br>
<img src="/blogs/joplin_resources/b2211e259db3d82c367ac958607b5efc.png" alt=""><br>
It is an amazing result, as LLM can evolve to a reasoning model without any supervised fine-tuning data. It underscores the model’s ability to<br>
learn and generalize effectively through RL alone.</p>
<h4 id="DeepSeek-R1">DeepSeek-R1</h4>
<p>The authors further propose two questions:</p>
<ol>
<li>Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start?</li>
<li>How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?</li>
</ol>
<p>To solve that, they propose a novel pipeline to train DeepSeek-R1:</p>
<ul>
<li>Cold Start<br>
They construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor, by:
<ul>
<li>using few-shot prompting with a long CoT as an example;</li>
<li>directly prompting the model to generate detailed output with reflection and verification;</li>
<li>filtering the Zero output, to ensure readability and refine the output by human annotators.</li>
</ul>
</li>
<li>Reasoning-oriented Reinforcement Learning<br>
After fine-tuning on the cold start data, they combine the following rewards for the RL training.
<ul>
<li>Language consistency reward: the proportion of target language words in the CoT;</li>
<li>Accuracy reward like above.</li>
</ul>
</li>
<li>Rejection Sampling and Supervised Fine-Tuning
<ul>
<li>Perform rejection sampling from the above RL training checkpoint to collect fine-tuning data. To evaluate the quality of the data, in addition to the rule-based rewards like accuracy, this step also considers a generative rewards. Particularly, for the prompt, they feed the ground truth and the output of the model (here, I think they are referring to the model after the above RL training) to DeepSeek-V3 for judgement (e.g., I guess to give some match score). They collect 600K reasoning related training samples by doing this step.</li>
<li>They also collect 200K non-reasoning samples from the SFT dataset of DeepSeek-V3.<br>
After that, they perform fine-tuning for two epochs using the 800K.</li>
</ul>
</li>
<li>Reinforcement Learning for all Scenarios<br>
They train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios.</li>
</ul>
<p>After the above four steps, R1 can be obtained.</p>
<p>About distillation, they use R1 to curate the 800K samples and use the curated data to fine-tune (no RL here) other open-source LLMs, the performances are shown below:<br>
<img src="/blogs/joplin_resources/afbdd712cf7c486ccb3940d3185412ab.png" alt=""></p>
<p>They also state that incorporating RL could substantially boost model performance.</p>
<h3 id="Limitations">Limitations</h3>
<ul>
<li>Limitations in function calling, multi-turn, complex role-playing, JSON output, etc.</li>
<li>Currently optimized for Chinese and English, not good for other languages.</li>
<li>sensitive to prompts. Few-shot prompting consistently degrades its performance. Recommend to use zero-shot but detailed prompt.</li>
<li>has not been applied extensively in software engineering tasks.</li>
</ul>
]]></content>
      <categories>
        <category>Paper reading</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>DeepSeek</tag>
      </tags>
  </entry>
</search>
