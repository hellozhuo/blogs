<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/blogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blogs/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blogs/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhuogege1943.com","root":"/blogs/","images":"/blogs/images","scheme":"Gemini","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/blogs/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/blogs/js/config.js"></script>

    <meta name="description" content="ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper (2019 arxiv, SC’20): https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.02054 website: https:&#x2F;&#x2F;www.deepspeed.ai&#x2F;">
<meta property="og:type" content="article">
<meta property="og:title" content="ZeRO (DeepSpeed from Microsoft)">
<meta property="og:url" content="https://zhuogege1943.com/blogs/2025/01/01/ZeRO-DeepSpeed-from-Microsoft/index.html">
<meta property="og:site_name" content="Zhuo&#39;s Blog">
<meta property="og:description" content="ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper (2019 arxiv, SC’20): https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.02054 website: https:&#x2F;&#x2F;www.deepspeed.ai&#x2F;">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhuogege1943.com/blogs/joplin_resources/d7080c98a06ed87132146d152825625a.png">
<meta property="og:image" content="https://zhuogege1943.com/blogs/joplin_resources/5a8f4be1364e8b6e41e42537eb06c768.png">
<meta property="og:image" content="https://zhuogege1943.com/blogs/joplin_resources/8557e921935a44025263389ac9a74fc6.png">
<meta property="og:image" content="https://zhuogege1943.com/blogs/joplin_resources/8fdb71dae74d76259cc162100ddd3d80.png">
<meta property="og:image" content="https://zhuogege1943.com/blogs/joplin_resources/dd7fc0c7103eb20c0510374bd42e44a6.png">
<meta property="article:published_time" content="2025-01-01T09:04:06.000Z">
<meta property="article:modified_time" content="2025-01-13T21:54:30.732Z">
<meta property="article:author" content="Zhuo ge ge">
<meta property="article:tag" content="Optimization">
<meta property="article:tag" content="Distributed training">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhuogege1943.com/blogs/joplin_resources/d7080c98a06ed87132146d152825625a.png">


<link rel="canonical" href="https://zhuogege1943.com/blogs/2025/01/01/ZeRO-DeepSpeed-from-Microsoft/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://zhuogege1943.com/blogs/2025/01/01/ZeRO-DeepSpeed-from-Microsoft/","path":"2025/01/01/ZeRO-DeepSpeed-from-Microsoft/","title":"ZeRO (DeepSpeed from Microsoft)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ZeRO (DeepSpeed from Microsoft) | Zhuo's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/blogs/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Zhuo's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blogs/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/blogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/blogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-all-blogs"><a href="/blogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>All blogs</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models"><span class="nav-number">1.</span> <span class="nav-text">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Overivew-of-ZeRO-optimizers"><span class="nav-number">1.1.</span> <span class="nav-text">Overivew of ZeRO optimizers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Let%E2%80%99s-see-which-occupies-the-GPU-memory-during-training"><span class="nav-number">1.2.</span> <span class="nav-text">Let’s see which occupies the GPU memory during training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZeRO-DP-to-reduce-the-first-three-memory-parts-parameters-gradients-and-optimizer-states"><span class="nav-number">1.3.</span> <span class="nav-text">ZeRO-DP to reduce the first three memory parts (parameters, gradients, and optimizer states)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-DP-P-os"><span class="nav-number">1.3.1.</span> <span class="nav-text">ZeRO-DP $P_{os}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-DP-P-os-g"><span class="nav-number">1.3.2.</span> <span class="nav-text">ZeRO-DP $P_{os+g}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-DP-P-os-g-p"><span class="nav-number">1.3.3.</span> <span class="nav-text">ZeRO-DP $P_{os+g+p}$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZeRO-R-to-reduce-residual-memry"><span class="nav-number">1.4.</span> <span class="nav-text">ZeRO-R to reduce residual memry</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-R-P-a-and-P-a-cpu"><span class="nav-number">1.4.1.</span> <span class="nav-text">ZeRO-R $P_a$ and $P_{a+cpu}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-R-C-B-and-M-D"><span class="nav-number">1.4.2.</span> <span class="nav-text">ZeRO-R $C_B$ and $M_D$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conlusion"><span class="nav-number">1.5.</span> <span class="nav-text">Conlusion</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <a href="https://zhuogege1943.com" title=" → personal homepage">
    <img class="site-author-image" itemprop="image" alt="Zhuo ge ge"
      src="/blogs/images/dushen.png">
    </a>
  <p class="site-author-name" itemprop="name">Zhuo ge ge</p>
  <div class="site-description" itemprop="description">Hi, nice to meet you</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blogs/archives/">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blogs/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/blogs/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hellozhuo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hellozhuo" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zuike2013@outlook.com" title="E-Mail → mailto:zuike2013@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhuogege1943.com/blogs/2025/01/01/ZeRO-DeepSpeed-from-Microsoft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blogs/images/dushen.png">
      <meta itemprop="name" content="Zhuo ge ge">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuo's Blog">
      <meta itemprop="description" content="Hi, nice to meet you">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ZeRO (DeepSpeed from Microsoft) | Zhuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ZeRO (DeepSpeed from Microsoft)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-01 11:04:06" itemprop="dateCreated datePublished" datetime="2025-01-01T11:04:06+02:00">2025-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-13 23:54:30" itemprop="dateModified" datetime="2025-01-13T23:54:30+02:00">2025-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blogs/categories/Paper-reading/" itemprop="url" rel="index"><span itemprop="name">Paper reading</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</h2>
<p>paper (2019 arxiv, SC’20): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a><br>
website: <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/">https://www.deepspeed.ai/</a></p>
<span id="more"></span>
<h3 id="Overivew-of-ZeRO-optimizers">Overivew of ZeRO optimizers</h3>
<p><img src="/blogs/joplin_resources/d7080c98a06ed87132146d152825625a.png" alt=""></p>
<p>So basically, ZeRO has two sets of optimizations: <code>ZeRO-DP</code> to reduce the memory footprint along with data parallelism (like optimizer states by $P_{os}$, gradients by $P_g$, and parameters by $P_p$), and <code>ZeRO-R</code> to reduce the residual memory (like activations by $P_a$, temporary buffers by $C_B$, and memory fragementation by $M_D$). Particularly, activation are reduced along with model parallelism (I would say might be implemented as “sequence parallelism” in Megagron).</p>
<p>The highlight is that ZeRO-DP can significantly reduce the memory per GPU while keeping the communication overhead between GPUs unchanged when only applying DP alone (without the need of MP), therefore allow bigger models to train efficiently.<br>
<img src="/blogs/joplin_resources/5a8f4be1364e8b6e41e42537eb06c768.png" alt=""></p>
<p>For example, as shown in above figure, Megagron-LM (which only applying tensor model parallelism here) will need to use multiple nodes because a single node (16 GPUs) cannot fit a bigger model (larger than 40B), causing inter-node communications that are very slow. However, for ZeRO, MP always fits in a node. Megatron make the computation granularity smaller (by splitting the model into smaller pieces) while ZeRO keeps every whole layer of transformer (my opinion). In addition, ZeRO also performs better than Megatron becuase the memory on each DP GPU is reduced without increasing communication overhead (as can be seen in the 1.5B case).</p>
<p>By adjusing the degree of ZeRO-DP, degree of MP, batch size, etc, ZeRO can gives good scalability for large models, even super-linear scalability:<br>
<img src="/blogs/joplin_resources/8557e921935a44025263389ac9a74fc6.png" alt=""><br>
It is because $P{os+g}$ reduces per GPU memory consumption of ZeRO-100B with increase in DP degree, allowing ZeRO-100B to fit larger batch sizes per GPU, which in turn improves throughput as a result of increasing arithmetic intensity. For example, we assume MP is fit in a node with 16 GPUs, thus the degree of MP is bounded to 16, then the degree of DP can be increased with more GPUs.</p>
<p>So let’s describe more about the methods.</p>
<h3 id="Let’s-see-which-occupies-the-GPU-memory-during-training">Let’s see which occupies the GPU memory during training</h3>
<p>Let’s assume the training is via mixed precision (fp16/32) training where the parameters and gradients are stored in 16 bit (2bytes) while the optimizer states use 32-bit (4bytes). Let the number of parameters is $\Psi$ and Adam optimizer is used, then here is the memory consumption:</p>
<ul>
<li>Parameters: $2\Psi$</li>
<li>Gradients: $2\Psi$</li>
<li>Optimizer states: parameters + momentum + variance cuase $4\Psi + 4\Psi + 4\Psi = 12\Psi$</li>
<li>Activations: It depends on the sequence length, batch size, and the structure of transformer models (please also see <a href="/blogs/2024/12/26/Megatron-LM-3">Megatron-LM series 3</a>). For example, GPT-2 with 1.5B parameters, 1K sequence length, and batch size 32 causes 60G memory.</li>
<li>Temporary buffers: Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput.</li>
<li>Memory fragmentation: It is caused by the interleaving between short lived and long lived memory objects. For example, the activations checkpointed (uisng activation checkpointing) lives longer than those need to be recomputed during forward pass. The gradients of parameters live longer than the gradients of activations during backward pass. A long contiguous memory will be fragmented into small pieces when the interleaved short memory has been released. So it will still possible to cause OOM error when the total available memory is larger than requested because of the lack of longer enough contiguous memory.</li>
</ul>
<h3 id="ZeRO-DP-to-reduce-the-first-three-memory-parts-parameters-gradients-and-optimizer-states">ZeRO-DP to reduce the first three memory parts (parameters, gradients, and optimizer states)</h3>
<p><img src="/blogs/joplin_resources/8fdb71dae74d76259cc162100ddd3d80.png" alt=""><br>
As shown in the above figure, ZeRO has three stages $P_{os}$, $P_{os+g}$, and $P{os+g+p}$ that gradually reduce the memory consumed per GPU ($N_d$ is the degree of data parallelism).</p>
<h4 id="ZeRO-DP-P-os">ZeRO-DP $P_{os}$</h4>
<p>For $P_{os}$, the idea is quite simple. In this case, only the optimizer states are partioned to different DP GPUs, but all the parameters are replicated in each GPU. During forward and backward passes, the each DP GPU run for its own minibatch and get the loss via forward pass and gradient via backward pass. Then the gradients are kind of processed with “reduce-scatter” to the optimizers in each GPU (first reduce the gradients to calculate the average, and scatter the gradients to different pieces and give each GPU a piece). After that, each GPU run optimizer.step() to update the corresponding parameters. Finally, the “all-gather” operation is used to gather different pieces of updated parameters as the new parameters on each GPU.</p>
<blockquote>
<p>Communication overhead<br>
Compared with the original way of DP, the “reduce-scatter” and “all-gather” operations cause the same communication overhead as the “all-reduce” operation in the original DP.</p>
</blockquote>
<h4 id="ZeRO-DP-P-os-g">ZeRO-DP $P_{os+g}$</h4>
<p>For gradients, the forward pass is the same as above, but for the backward pass, whenever the gradients of a certain part of the parameters are calculated, use “reduction” operation for the gradients on different GPUs to get the average and store the average on the associated GPU only. After backward, each GPU has its own gradients part for a certain par of parameters, means the GPU can directly apply optimizer.step() to update those parameters. After tha, “all-gather” is used to update all parameters on each GPU.</p>
<blockquote>
<p>Communication overhead<br>
Put all the “reduction” operations together gives the same amount of communication as the “reduce-scatter” operation based on all gradient in $P_{os}$, plus “all-gather”, it give the same communication as the original DP.</p>
</blockquote>
<h4 id="ZeRO-DP-P-os-g-p">ZeRO-DP $P_{os+g+p}$</h4>
<p>Each DP GPU only store a certain part of parameters further. When the parameters outside of its partition are required for forward and backward propagation, they are received from the appropriate data parallel process through broadcast. The rest is the same as $P_{os+g}$.</p>
<blockquote>
<p>Coummnication overhead<br>
As stated by the authors, these extra broadcast operations cuases $1.5\times$ overhead compared with the previous two cases.</p>
</blockquote>
<p>When $P_{os+g+p}$ is applied, the memory per GPU can be reduced by $N_d\times$, enabling very large models (1 trillion parameters) to fit in the GPU cluster with 1024 GPUs:</p>
<p><img src="/blogs/joplin_resources/dd7fc0c7103eb20c0510374bd42e44a6.png" alt=""><br>
Here, with 1024 DP, each GPU only consumes 15.6 G memory for a 1 T model, which is lower than its limit (i.e., 32G for V100).</p>
<h3 id="ZeRO-R-to-reduce-residual-memry">ZeRO-R to reduce residual memry</h3>
<h4 id="ZeRO-R-P-a-and-P-a-cpu">ZeRO-R $P_a$ and $P_{a+cpu}$</h4>
<p>In my opinion, $P_a$ would be similar to <code>sequence parallelism</code> in Megatron-LM, but ZeRO might partition the activations along different dimensions along MP. Then $P_{a+cpu}$ additionally store the activations in CPU to free GPU memory.</p>
<h4 id="ZeRO-R-C-B-and-M-D">ZeRO-R $C_B$ and $M_D$</h4>
<p>For $C_B$, they  simply use a performance-efficient constant-size fused buffer when the model becomes too large. For $M_D$, ZeRO does memory defragmentation on-the-fly by pre-allocating contiguous memory chunks for activation checkpoints and gradients, and copying them over to the pre-allocated memory as they are produced. MD not only enables ZeRO to train larger models with larger batch sizes, but also improves efficiency when training with limited memory.</p>
<h3 id="Conlusion">Conlusion</h3>
<p>ZeRO goes with DP, so from the perspective of model parameters, it doesn’t refactor the archiecture like MP. It can be combined with MP like Megatron to further scale the training while outperforming combining DP and MP alone.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/blogs/tags/Optimization/" rel="tag"># Optimization</a>
              <a href="/blogs/tags/Distributed-training/" rel="tag"># Distributed training</a>
              <a href="/blogs/tags/LLM/" rel="tag"># LLM</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blogs/2024/12/28/How-to-check-the-blog-webpages-in-our-local-comput/" rel="prev" title="How to check remote hexo webpages in our local computer">
                  <i class="fa fa-angle-left"></i> How to check remote hexo webpages in our local computer
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/blogs/2025/01/13/ZeRO-Offload/" rel="next" title="ZeRO-Offload">
                  ZeRO-Offload <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zhuo ge ge</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blogs/js/comments.js"></script><script src="/blogs/js/utils.js"></script><script src="/blogs/js/motion.js"></script><script src="/blogs/js/sidebar.js"></script><script src="/blogs/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/blogs/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/blogs/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"hellozhuo/blogs","issue_term":"pathname","theme":"preferred-color-scheme"}</script>
<script src="/blogs/js/third-party/comments/utterances.js"></script>

</body>
</html>
